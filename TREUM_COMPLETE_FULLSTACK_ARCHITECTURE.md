# TREUM ALGOTECH - Complete Full-Stack Architecture
## Frontend, Brownfield & Integration Architecture

---

## Executive Summary

This document complements the existing backend architecture by providing comprehensive specifications for:

1. **Frontend Architecture** - Modern, scalable client-side architecture
2. **Brownfield Integration** - Legacy system integration strategies
3. **Integration Patterns** - How all components work together

**Scale Target**: â‚¹600 Cr revenue, 1M+ concurrent users, financial-grade compliance

---

## 1. Frontend Architecture

### 1.1 Component-Based Architecture Design

**Architecture Philosophy**:
- **Atomic Design**: Components built from atoms â†’ molecules â†’ organisms â†’ templates â†’ pages
- **Domain-Driven Design**: Feature modules aligned with business domains
- **Micro-Frontend Ready**: Loosely coupled modules for independent deployment
- **Performance First**: Bundle optimization, code splitting, lazy loading

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     TREUM FRONTEND ARCHITECTURE                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Presentation Layer                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Web App (Next.js)  â”‚  Mobile (RN)  â”‚  Admin (React+Vite) â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚  â”‚ Education Hub   â”‚ â”‚ â”‚Trading Appâ”‚ â”‚ â”‚Analytics Panel â”‚ â”‚ â”‚
â”‚  â”‚  â”‚ Signals Portal  â”‚ â”‚ â”‚Profile Mgmtâ”‚ â”‚ â”‚User Management â”‚ â”‚ â”‚
â”‚  â”‚  â”‚ Payment Flow    â”‚ â”‚ â”‚Live Signalsâ”‚ â”‚ â”‚Content CMS     â”‚ â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  State Management Layer                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Redux Toolkit + RTK Query â”‚ Zustand â”‚ React Query + SWR   â”‚ â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚ â”‚Global App State       â”‚ â”‚ â”‚Localâ”‚ â”‚ â”‚Server State     â”‚ â”‚ â”‚
â”‚  â”‚ â”‚User, Auth, UI Theme   â”‚ â”‚ â”‚Stateâ”‚ â”‚ â”‚API Data Cache   â”‚ â”‚ â”‚
â”‚  â”‚ â”‚Signals, Notifications â”‚ â”‚ â”‚ Mgmtâ”‚ â”‚ â”‚Real-time Updatesâ”‚ â”‚ â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Component Library & Design System                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ TREUM-UI Design System â”‚ Component Library â”‚ Theme Engine  â”‚ â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚ â”‚Tokens & Primitives  â”‚ â”‚ â”‚Reusable Comps â”‚ â”‚ â”‚Multi-themeâ”‚ â”‚ â”‚
â”‚  â”‚ â”‚Colors, Typography   â”‚ â”‚ â”‚Forms, Tables  â”‚ â”‚ â”‚Dark/Light â”‚ â”‚ â”‚
â”‚  â”‚ â”‚Spacing, Breakpoints â”‚ â”‚ â”‚Charts, Modals â”‚ â”‚ â”‚Custom Brandâ”‚ â”‚ â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Real-time & Communication Layer                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ WebSocket Manager â”‚ Push Notifications â”‚ Service Workers   â”‚ â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚ â”‚Signal Streams â”‚ â”‚ â”‚FCM Integration â”‚ â”‚ â”‚Offline Supportâ”‚ â”‚ â”‚
â”‚  â”‚ â”‚Live Price Feedâ”‚ â”‚ â”‚Email/SMS Queue â”‚ â”‚ â”‚Background Syncâ”‚ â”‚ â”‚
â”‚  â”‚ â”‚Chat Support  â”‚ â”‚ â”‚In-app Messages â”‚ â”‚ â”‚Cache Strategy â”‚ â”‚ â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 Micro-Frontend Architecture

**Module Decomposition Strategy**:

```yaml
Micro-Frontend Modules:

Education Module:
  Route: /education/*
  Technology: Next.js 14 + React 18
  Bundle Size: < 500KB gzipped
  Features:
    - Course catalog and search
    - Video player with progress tracking
    - Assessment and quiz system
    - Certificate generation
    - Discussion forums
  
Signals Module:
  Route: /signals/*
  Technology: Vite + React 18 + WebSocket
  Bundle Size: < 300KB gzipped
  Features:
    - Real-time signal feed
    - Signal history and analytics
    - Portfolio tracking
    - Alert management
    - Performance metrics
    
Trading Module:
  Route: /trading/*
  Technology: Next.js 14 + TradingView
  Bundle Size: < 800KB gzipped
  Features:
    - Exchange integration
    - Order management
    - Portfolio dashboard
    - Risk management
    - Trade history
    
Payment Module:
  Route: /payments/*
  Technology: Secure iframe + React
  Bundle Size: < 200KB gzipped
  Features:
    - Payment gateway integration
    - Subscription management
    - Invoice generation
    - Refund handling
    - Compliance reporting

Profile Module:
  Route: /profile/*
  Technology: React 18 + Formik
  Bundle Size: < 250KB gzipped
  Features:
    - User profile management
    - KYC document upload
    - Preferences and settings
    - Security settings
    - Notification preferences
```

**Module Federation Setup**:

```javascript
// webpack.config.js for Module Federation
const ModuleFederationPlugin = require('@module-federation/webpack');

module.exports = {
  mode: 'production',
  plugins: [
    new ModuleFederationPlugin({
      name: 'shell',
      remotes: {
        education: 'education@https://education.treum.in/remoteEntry.js',
        signals: 'signals@https://signals.treum.in/remoteEntry.js',
        trading: 'trading@https://trading.treum.in/remoteEntry.js',
        payments: 'payments@https://payments.treum.in/remoteEntry.js',
        profile: 'profile@https://profile.treum.in/remoteEntry.js'
      },
      shared: {
        react: { singleton: true, version: '18.2.0' },
        'react-dom': { singleton: true, version: '18.2.0' },
        '@treum/ui-kit': { singleton: true },
        '@treum/state-manager': { singleton: true },
        '@treum/utils': { singleton: true }
      }
    })
  ]
};

// Shell Application Router
import { Suspense, lazy } from 'react';
import { BrowserRouter, Routes, Route } from 'react-router-dom';
import { ErrorBoundary } from '@treum/ui-kit';
import { LoadingSpinner } from '@treum/ui-kit';

// Lazy load micro-frontends
const EducationApp = lazy(() => import('education/App'));
const SignalsApp = lazy(() => import('signals/App'));
const TradingApp = lazy(() => import('trading/App'));
const PaymentsApp = lazy(() => import('payments/App'));
const ProfileApp = lazy(() => import('profile/App'));

function AppRouter() {
  return (
    <BrowserRouter>
      <Routes>
        <Route 
          path="/education/*" 
          element={
            <ErrorBoundary fallback={<ErrorPage />}>
              <Suspense fallback={<LoadingSpinner />}>
                <EducationApp />
              </Suspense>
            </ErrorBoundary>
          } 
        />
        <Route 
          path="/signals/*" 
          element={
            <ErrorBoundary fallback={<ErrorPage />}>
              <Suspense fallback={<LoadingSpinner />}>
                <SignalsApp />
              </Suspense>
            </ErrorBoundary>
          } 
        />
        {/* Additional routes... */}
      </Routes>
    </BrowserRouter>
  );
}
```

### 1.3 State Management Architecture

**Multi-Layer State Strategy**:

```typescript
// State Architecture Design
interface StateArchitecture {
  global: GlobalState;      // Redux Toolkit - App-wide state
  server: ServerState;      // RTK Query/React Query - API data
  local: LocalState;        // useState/useReducer - Component state
  url: URLState;           // React Router - Navigation state
  persistent: PersistentState; // localStorage/sessionStorage
}

// Global State (Redux Toolkit)
interface GlobalState {
  auth: {
    user: User | null;
    tokens: { access: string; refresh: string } | null;
    isAuthenticated: boolean;
    permissions: Permission[];
  };
  ui: {
    theme: 'light' | 'dark' | 'auto';
    sidebarCollapsed: boolean;
    notifications: Notification[];
    modals: ModalState[];
  };
  realtime: {
    socketConnected: boolean;
    signalsFeed: Signal[];
    priceUpdates: PriceUpdate[];
    connectionStatus: 'connected' | 'connecting' | 'disconnected';
  };
  feature: {
    education: EducationState;
    trading: TradingState;
    signals: SignalsState;
  };
}

// Redux Store Configuration
import { configureStore } from '@reduxjs/toolkit';
import { setupListeners } from '@reduxjs/toolkit/query';
import { persistStore, persistReducer } from 'redux-persist';
import storage from 'redux-persist/lib/storage';

// API Slice for server state
import { treuMAPISlice } from './api/treuMAPISlice';

// Feature slices
import authSlice from './slices/authSlice';
import uiSlice from './slices/uiSlice';
import realtimeSlice from './slices/realtimeSlice';

const persistConfig = {
  key: 'root',
  storage,
  whitelist: ['auth', 'ui'], // Only persist auth and UI state
  blacklist: ['api', 'realtime'] // Don't persist API and realtime data
};

const rootReducer = combineReducers({
  auth: authSlice.reducer,
  ui: uiSlice.reducer,
  realtime: realtimeSlice.reducer,
  api: treuMAPISlice.reducer,
});

const persistedReducer = persistReducer(persistConfig, rootReducer);

export const store = configureStore({
  reducer: persistedReducer,
  middleware: (getDefaultMiddleware) =>
    getDefaultMiddleware({
      serializableCheck: {
        ignoredActions: [FLUSH, REHYDRATE, PAUSE, PERSIST, PURGE, REGISTER],
      },
    }).concat(treuMAPISlice.middleware),
  devTools: process.env.NODE_ENV !== 'production',
});

// Server State Management (RTK Query)
import { createApi, fetchBaseQuery } from '@reduxjs/toolkit/query/react';

export const treuMAPISlice = createApi({
  reducerPath: 'api',
  baseQuery: fetchBaseQuery({
    baseUrl: '/api/v1',
    prepareHeaders: (headers, { getState }) => {
      const token = (getState() as RootState).auth.tokens?.access;
      if (token) {
        headers.set('authorization', `Bearer ${token}`);
      }
      return headers;
    },
  }),
  tagTypes: ['User', 'Course', 'Signal', 'Transaction', 'Subscription'],
  endpoints: (builder) => ({
    // Authentication
    login: builder.mutation<LoginResponse, LoginRequest>({
      query: (credentials) => ({
        url: '/auth/login',
        method: 'POST',
        body: credentials,
      }),
      invalidatesTags: ['User'],
    }),

    // User Profile
    getProfile: builder.query<User, void>({
      query: () => '/users/profile',
      providesTags: ['User'],
    }),

    updateProfile: builder.mutation<User, Partial<User>>({
      query: (updates) => ({
        url: '/users/profile',
        method: 'PUT',
        body: updates,
      }),
      invalidatesTags: ['User'],
    }),

    // Courses
    getCourses: builder.query<CoursesResponse, CoursesQuery>({
      query: (params) => ({
        url: '/courses',
        params,
      }),
      providesTags: ['Course'],
    }),

    enrollInCourse: builder.mutation<EnrollmentResponse, EnrollmentRequest>({
      query: ({ courseId, paymentData }) => ({
        url: `/courses/${courseId}/enroll`,
        method: 'POST',
        body: paymentData,
      }),
      invalidatesTags: ['Course', 'User', 'Transaction'],
    }),

    // Signals
    getLiveSignals: builder.query<Signal[], void>({
      query: () => '/signals/live',
      providesTags: ['Signal'],
      // Polling for live updates
      pollingInterval: 5000,
    }),

    getSignalHistory: builder.query<SignalsResponse, SignalHistoryQuery>({
      query: (params) => ({
        url: '/signals/history',
        params,
      }),
      providesTags: ['Signal'],
    }),

    // Payments
    createPayment: builder.mutation<PaymentResponse, PaymentRequest>({
      query: (paymentData) => ({
        url: '/payments',
        method: 'POST',
        body: paymentData,
      }),
      invalidatesTags: ['Transaction'],
    }),

    // Subscriptions
    getSubscriptions: builder.query<Subscription[], void>({
      query: () => '/subscriptions',
      providesTags: ['Subscription'],
    }),
  }),
});

// Export hooks for use in components
export const {
  useLoginMutation,
  useGetProfileQuery,
  useUpdateProfileMutation,
  useGetCoursesQuery,
  useEnrollInCourseMutation,
  useGetLiveSignalsQuery,
  useGetSignalHistoryQuery,
  useCreatePaymentMutation,
  useGetSubscriptionsQuery,
} = treuMAPISlice;
```

### 1.4 Real-time Data Architecture

**WebSocket Integration Strategy**:

```typescript
// Real-time WebSocket Manager
class WebSocketManager {
  private ws: WebSocket | null = null;
  private reconnectAttempts = 0;
  private maxReconnectAttempts = 5;
  private reconnectInterval = 1000;
  private pingInterval: NodeJS.Timeout | null = null;
  private subscriptions = new Map<string, Set<(data: any) => void>>();

  constructor(private store: AppStore) {}

  connect(token: string) {
    const wsUrl = process.env.NEXT_PUBLIC_WS_URL || 'wss://api.treum.in/ws';
    
    try {
      this.ws = new WebSocket(`${wsUrl}?token=${token}`);
      this.setupEventHandlers();
      this.startPingPong();
    } catch (error) {
      console.error('WebSocket connection failed:', error);
      this.scheduleReconnect();
    }
  }

  private setupEventHandlers() {
    if (!this.ws) return;

    this.ws.onopen = () => {
      console.log('WebSocket connected');
      this.reconnectAttempts = 0;
      this.store.dispatch(setConnectionStatus('connected'));
      
      // Subscribe to user-specific channels
      this.subscribeToChannels();
    };

    this.ws.onmessage = (event) => {
      try {
        const message = JSON.parse(event.data);
        this.handleMessage(message);
      } catch (error) {
        console.error('Failed to parse WebSocket message:', error);
      }
    };

    this.ws.onclose = (event) => {
      console.log('WebSocket disconnected:', event.code, event.reason);
      this.store.dispatch(setConnectionStatus('disconnected'));
      this.cleanup();
      
      if (event.code !== 1000) { // Not a normal closure
        this.scheduleReconnect();
      }
    };

    this.ws.onerror = (error) => {
      console.error('WebSocket error:', error);
      this.store.dispatch(setConnectionStatus('disconnected'));
    };
  }

  private handleMessage(message: WebSocketMessage) {
    const { type, channel, data } = message;

    switch (type) {
      case 'SIGNAL_UPDATE':
        this.store.dispatch(addSignal(data));
        this.notifySubscribers('signals', data);
        break;

      case 'PRICE_UPDATE':
        this.store.dispatch(updatePrice(data));
        this.notifySubscribers('prices', data);
        break;

      case 'NOTIFICATION':
        this.store.dispatch(addNotification(data));
        this.notifySubscribers('notifications', data);
        break;

      case 'USER_UPDATE':
        this.store.dispatch(updateUser(data));
        break;

      case 'PORTFOLIO_UPDATE':
        this.store.dispatch(updatePortfolio(data));
        this.notifySubscribers('portfolio', data);
        break;

      case 'SYSTEM_MESSAGE':
        this.handleSystemMessage(data);
        break;

      default:
        console.warn('Unknown message type:', type);
    }
  }

  subscribe(channel: string, callback: (data: any) => void) {
    if (!this.subscriptions.has(channel)) {
      this.subscriptions.set(channel, new Set());
    }
    this.subscriptions.get(channel)!.add(callback);

    // Send subscription request to server
    this.send({
      type: 'SUBSCRIBE',
      channel,
    });

    // Return unsubscribe function
    return () => {
      const callbacks = this.subscriptions.get(channel);
      if (callbacks) {
        callbacks.delete(callback);
        if (callbacks.size === 0) {
          this.subscriptions.delete(channel);
          this.send({
            type: 'UNSUBSCRIBE',
            channel,
          });
        }
      }
    };
  }

  private notifySubscribers(channel: string, data: any) {
    const callbacks = this.subscriptions.get(channel);
    if (callbacks) {
      callbacks.forEach(callback => callback(data));
    }
  }

  send(message: any) {
    if (this.ws && this.ws.readyState === WebSocket.OPEN) {
      this.ws.send(JSON.stringify(message));
    } else {
      console.warn('WebSocket not connected, message not sent:', message);
    }
  }

  private subscribeToChannels() {
    const state = this.store.getState();
    const userId = state.auth.user?.id;
    const subscription = state.auth.user?.subscription;

    if (userId) {
      // Subscribe to user-specific channels
      this.send({
        type: 'SUBSCRIBE',
        channel: `user_${userId}`,
      });

      // Subscribe to signals based on subscription tier
      if (subscription?.active) {
        this.send({
          type: 'SUBSCRIBE',
          channel: `signals_${subscription.tier}`,
        });
      }

      // Subscribe to price updates for watchlist
      this.send({
        type: 'SUBSCRIBE',
        channel: 'prices_general',
      });
    }
  }

  private startPingPong() {
    this.pingInterval = setInterval(() => {
      if (this.ws && this.ws.readyState === WebSocket.OPEN) {
        this.send({ type: 'PING' });
      }
    }, 30000); // Ping every 30 seconds
  }

  private scheduleReconnect() {
    if (this.reconnectAttempts < this.maxReconnectAttempts) {
      setTimeout(() => {
        this.reconnectAttempts++;
        const token = this.store.getState().auth.tokens?.access;
        if (token) {
          this.connect(token);
        }
      }, this.reconnectInterval * Math.pow(2, this.reconnectAttempts));
    }
  }

  private cleanup() {
    if (this.pingInterval) {
      clearInterval(this.pingInterval);
      this.pingInterval = null;
    }
  }

  disconnect() {
    if (this.ws) {
      this.ws.close(1000, 'Client disconnect');
      this.cleanup();
    }
  }
}

// React Hook for WebSocket subscriptions
export function useWebSocket<T>(
  channel: string,
  enabled: boolean = true
): {
  data: T | null;
  isConnected: boolean;
  subscribe: (callback: (data: T) => void) => () => void;
} {
  const [data, setData] = useState<T | null>(null);
  const isConnected = useSelector((state: RootState) => 
    state.realtime.connectionStatus === 'connected'
  );
  const wsManager = useWebSocketManager();

  useEffect(() => {
    if (!enabled || !isConnected) return;

    const unsubscribe = wsManager.subscribe(channel, (newData: T) => {
      setData(newData);
    });

    return unsubscribe;
  }, [channel, enabled, isConnected, wsManager]);

  const subscribe = useCallback(
    (callback: (data: T) => void) => wsManager.subscribe(channel, callback),
    [channel, wsManager]
  );

  return { data, isConnected, subscribe };
}

// Usage in components
function SignalsFeed() {
  const { data: liveSignals, isConnected } = useWebSocket<Signal[]>('signals', true);
  const [historicalSignals] = useGetLiveSignalsQuery();

  return (
    <div className="signals-feed">
      <div className="connection-status">
        {isConnected ? (
          <span className="connected">ğŸŸ¢ Live</span>
        ) : (
          <span className="disconnected">ğŸ”´ Reconnecting...</span>
        )}
      </div>
      
      {liveSignals?.map(signal => (
        <SignalCard key={signal.id} signal={signal} />
      ))}
    </div>
  );
}
```

### 1.5 Performance Optimization Strategy

**Code Splitting & Bundle Optimization**:

```typescript
// Dynamic Imports & Code Splitting
// 1. Route-level code splitting
const EducationModule = lazy(() => 
  import(/* webpackChunkName: "education" */ './modules/education')
);

const SignalsModule = lazy(() => 
  import(/* webpackChunkName: "signals" */ './modules/signals')
);

const TradingModule = lazy(() => 
  import(/* webpackChunkName: "trading" */ './modules/trading')
);

// 2. Component-level code splitting
const TradingViewChart = lazy(() => 
  import(/* webpackChunkName: "trading-view" */ './components/TradingViewChart')
);

const VideoPlayer = lazy(() => 
  import(/* webpackChunkName: "video-player" */ './components/VideoPlayer')
);

// 3. Vendor library code splitting
const ChartsLibrary = lazy(() => 
  import(/* webpackChunkName: "charts" */ 'react-chartjs-2')
);

// Webpack Bundle Analyzer Configuration
const withBundleAnalyzer = require('@next/bundle-analyzer')({
  enabled: process.env.ANALYZE === 'true',
});

module.exports = withBundleAnalyzer({
  // Next.js config
  experimental: {
    optimizeCss: true,
    optimizeImages: true,
    modern: true,
  },
  
  webpack: (config, { dev, isServer }) => {
    // Bundle optimization
    if (!dev && !isServer) {
      config.optimization.splitChunks = {
        chunks: 'all',
        cacheGroups: {
          vendor: {
            test: /[\\/]node_modules[\\/]/,
            name: 'vendors',
            chunks: 'all',
            priority: 10,
          },
          tradingView: {
            test: /[\\/]tradingview[\\/]/,
            name: 'tradingview',
            chunks: 'all',
            priority: 20,
          },
          common: {
            name: 'common',
            minChunks: 2,
            chunks: 'all',
            priority: 5,
            reuseExistingChunk: true,
          },
        },
      };
    }

    return config;
  },
});

// Progressive Loading Strategy
interface ProgressiveLoadingProps {
  priority: 'high' | 'medium' | 'low';
  children: React.ReactNode;
  fallback?: React.ReactNode;
  threshold?: number;
}

function ProgressiveLoader({ 
  priority, 
  children, 
  fallback = <LoadingSkeleton />,
  threshold = 0.1 
}: ProgressiveLoadingProps) {
  const [isInView, setIsInView] = useState(priority === 'high');
  const [isLoaded, setIsLoaded] = useState(false);
  const ref = useRef<HTMLDivElement>(null);

  useEffect(() => {
    if (priority === 'high') {
      setIsLoaded(true);
      return;
    }

    const observer = new IntersectionObserver(
      ([entry]) => {
        if (entry.isIntersecting) {
          setIsInView(true);
          observer.disconnect();
        }
      },
      { threshold }
    );

    if (ref.current) {
      observer.observe(ref.current);
    }

    return () => observer.disconnect();
  }, [priority, threshold]);

  useEffect(() => {
    if (isInView) {
      // Simulate loading delay based on priority
      const delay = priority === 'medium' ? 100 : 300;
      const timer = setTimeout(() => setIsLoaded(true), delay);
      return () => clearTimeout(timer);
    }
  }, [isInView, priority]);

  return (
    <div ref={ref}>
      {isLoaded ? children : fallback}
    </div>
  );
}

// Image Optimization Strategy
interface OptimizedImageProps {
  src: string;
  alt: string;
  width?: number;
  height?: number;
  priority?: boolean;
  quality?: number;
  sizes?: string;
}

function OptimizedImage({
  src,
  alt,
  width,
  height,
  priority = false,
  quality = 75,
  sizes = "100vw"
}: OptimizedImageProps) {
  const [isLoaded, setIsLoaded] = useState(false);
  const [hasError, setHasError] = useState(false);

  // Generate responsive srcSet
  const generateSrcSet = (baseSrc: string) => {
    const widths = [320, 640, 768, 1024, 1280, 1920];
    return widths
      .map(w => `${baseSrc}?w=${w}&q=${quality} ${w}w`)
      .join(', ');
  };

  return (
    <div className="relative overflow-hidden">
      {!isLoaded && !hasError && (
        <div className="absolute inset-0 bg-gray-200 animate-pulse" />
      )}
      
      <img
        src={`${src}?w=${width}&q=${quality}`}
        srcSet={generateSrcSet(src)}
        sizes={sizes}
        alt={alt}
        width={width}
        height={height}
        loading={priority ? "eager" : "lazy"}
        decoding="async"
        onLoad={() => setIsLoaded(true)}
        onError={() => setHasError(true)}
        className={`transition-opacity duration-300 ${
          isLoaded ? 'opacity-100' : 'opacity-0'
        }`}
      />
      
      {hasError && (
        <div className="absolute inset-0 flex items-center justify-center bg-gray-100">
          <span className="text-gray-500">Failed to load image</span>
        </div>
      )}
    </div>
  );
}
```

### 1.6 SEO & SSR/SSG Strategy

**Next.js SSR/SSG Implementation**:

```typescript
// SSG for Static Content (Course Catalog, Landing Pages)
export async function getStaticProps({ params }: GetStaticPropsContext) {
  try {
    // Generate static props for course pages
    const courseData = await getCourseData(params?.slug as string);
    const relatedCourses = await getRelatedCourses(courseData.category);
    
    return {
      props: {
        course: courseData,
        relatedCourses,
        // SEO metadata
        seo: {
          title: `${courseData.title} | TREUM Education`,
          description: courseData.description,
          keywords: courseData.tags.join(', '),
          image: courseData.thumbnail,
          url: `https://treum.in/courses/${courseData.slug}`,
        }
      },
      revalidate: 3600, // Revalidate every hour
    };
  } catch (error) {
    return {
      notFound: true,
      revalidate: 60, // Retry after 1 minute
    };
  }
}

export async function getStaticPaths() {
  // Generate paths for all courses
  const courses = await getAllCourses();
  
  const paths = courses.map((course) => ({
    params: { slug: course.slug },
  }));

  return {
    paths,
    fallback: 'blocking', // Enable ISR for new courses
  };
}

// SSR for Dynamic Content (User Dashboard, Live Signals)
export async function getServerSideProps({ req, res }: GetServerSidePropsContext) {
  // Set cache headers for dynamic content
  res.setHeader('Cache-Control', 'public, s-maxage=10, stale-while-revalidate=59');
  
  try {
    const token = extractTokenFromCookie(req.headers.cookie);
    
    if (!token) {
      return {
        redirect: {
          destination: '/login',
          permanent: false,
        },
      };
    }

    // Fetch user-specific data
    const [userProfile, userSignals, userCourses] = await Promise.all([
      getUserProfile(token),
      getUserSignals(token),
      getUserCourses(token),
    ]);

    return {
      props: {
        user: userProfile,
        signals: userSignals,
        courses: userCourses,
        // SSR metadata
        seo: {
          title: `Dashboard | ${userProfile.name} | TREUM`,
          description: 'Your personalized trading and education dashboard',
          robots: 'noindex, nofollow', // Private pages
        }
      },
    };
  } catch (error) {
    return {
      redirect: {
        destination: '/login',
        permanent: false,
      },
    };
  }
}

// SEO Component for Dynamic Meta Tags
interface SEOProps {
  title: string;
  description: string;
  keywords?: string;
  image?: string;
  url?: string;
  type?: 'website' | 'article' | 'profile';
  robots?: string;
  structuredData?: object;
}

function SEO({
  title,
  description,
  keywords,
  image = '/images/og-default.jpg',
  url,
  type = 'website',
  robots = 'index, follow',
  structuredData
}: SEOProps) {
  const router = useRouter();
  const canonicalUrl = url || `https://treum.in${router.asPath}`;

  return (
    <Head>
      {/* Basic Meta Tags */}
      <title>{title}</title>
      <meta name="description" content={description} />
      <meta name="keywords" content={keywords} />
      <meta name="robots" content={robots} />
      <link rel="canonical" href={canonicalUrl} />

      {/* Open Graph Tags */}
      <meta property="og:type" content={type} />
      <meta property="og:title" content={title} />
      <meta property="og:description" content={description} />
      <meta property="og:image" content={image} />
      <meta property="og:url" content={canonicalUrl} />
      <meta property="og:site_name" content="TREUM ALGOTECH" />

      {/* Twitter Card Tags */}
      <meta name="twitter:card" content="summary_large_image" />
      <meta name="twitter:title" content={title} />
      <meta name="twitter:description" content={description} />
      <meta name="twitter:image" content={image} />
      <meta name="twitter:site" content="@TreuMAlgotech" />

      {/* Additional Meta Tags */}
      <meta name="viewport" content="width=device-width, initial-scale=1" />
      <meta name="theme-color" content="#1a365d" />
      <meta name="msapplication-TileColor" content="#1a365d" />

      {/* Structured Data */}
      {structuredData && (
        <script
          type="application/ld+json"
          dangerouslySetInnerHTML={{
            __html: JSON.stringify(structuredData)
          }}
        />
      )}
    </Head>
  );
}

// Structured Data for Course Pages
function generateCourseStructuredData(course: Course) {
  return {
    "@context": "https://schema.org",
    "@type": "Course",
    "name": course.title,
    "description": course.description,
    "provider": {
      "@type": "Organization",
      "name": "TREUM ALGOTECH",
      "url": "https://treum.in"
    },
    "offers": {
      "@type": "Offer",
      "price": course.price,
      "priceCurrency": "INR",
      "availability": "https://schema.org/InStock"
    },
    "courseMode": "online",
    "educationalLevel": course.level,
    "instructor": {
      "@type": "Person",
      "name": course.instructor.name,
      "description": course.instructor.bio
    },
    "duration": `PT${course.durationMinutes}M`,
    "aggregateRating": course.rating ? {
      "@type": "AggregateRating",
      "ratingValue": course.rating.average,
      "reviewCount": course.rating.count
    } : undefined
  };
}
```

### 1.7 Progressive Web App (PWA) Implementation

**PWA Configuration**:

```typescript
// next.config.js PWA Setup
const withPWA = require('next-pwa')({
  dest: 'public',
  register: true,
  skipWaiting: true,
  runtimeCaching: [
    {
      urlPattern: /^https?.*/,
      handler: 'NetworkFirst',
      options: {
        cacheName: 'https-calls',
        networkTimeoutSeconds: 15,
        expiration: {
          maxEntries: 150,
          maxAgeSeconds: 30 * 24 * 60 * 60, // 30 days
        },
        cacheableResponse: {
          statuses: [0, 200],
        },
      },
    },
    {
      urlPattern: /\.(?:png|jpg|jpeg|svg|gif|webp)$/,
      handler: 'CacheFirst',
      options: {
        cacheName: 'images',
        expiration: {
          maxEntries: 100,
          maxAgeSeconds: 7 * 24 * 60 * 60, // 7 days
        },
      },
    },
    {
      urlPattern: /api\/.*\/*$/,
      handler: 'NetworkFirst',
      options: {
        cacheName: 'apis',
        networkTimeoutSeconds: 10,
        expiration: {
          maxEntries: 16,
          maxAgeSeconds: 24 * 60 * 60, // 24 hours
        },
        cacheableResponse: {
          statuses: [0, 200],
        },
      },
    },
  ],
});

module.exports = withPWA({
  // Next.js config
});

// manifest.json
{
  "name": "TREUM ALGOTECH - Trading & Education Platform",
  "short_name": "TREUM",
  "description": "AI-powered trading signals and comprehensive financial education",
  "start_url": "/",
  "display": "standalone",
  "background_color": "#ffffff",
  "theme_color": "#1a365d",
  "orientation": "portrait-primary",
  "categories": ["finance", "education", "productivity"],
  "lang": "en-IN",
  "icons": [
    {
      "src": "/icons/icon-72x72.png",
      "sizes": "72x72",
      "type": "image/png",
      "purpose": "maskable any"
    },
    {
      "src": "/icons/icon-96x96.png",
      "sizes": "96x96",
      "type": "image/png",
      "purpose": "maskable any"
    },
    {
      "src": "/icons/icon-128x128.png",
      "sizes": "128x128",
      "type": "image/png",
      "purpose": "maskable any"
    },
    {
      "src": "/icons/icon-144x144.png",
      "sizes": "144x144",
      "type": "image/png",
      "purpose": "maskable any"
    },
    {
      "src": "/icons/icon-152x152.png",
      "sizes": "152x152",
      "type": "image/png",
      "purpose": "maskable any"
    },
    {
      "src": "/icons/icon-192x192.png",
      "sizes": "192x192",
      "type": "image/png",
      "purpose": "maskable any"
    },
    {
      "src": "/icons/icon-384x384.png",
      "sizes": "384x384",
      "type": "image/png",
      "purpose": "maskable any"
    },
    {
      "src": "/icons/icon-512x512.png",
      "sizes": "512x512",
      "type": "image/png",
      "purpose": "maskable any"
    }
  ],
  "shortcuts": [
    {
      "name": "Live Signals",
      "short_name": "Signals",
      "description": "View live trading signals",
      "url": "/signals",
      "icons": [{ "src": "/icons/signals-96x96.png", "sizes": "96x96" }]
    },
    {
      "name": "My Courses",
      "short_name": "Courses",
      "description": "Access your enrolled courses",
      "url": "/education/my-courses",
      "icons": [{ "src": "/icons/education-96x96.png", "sizes": "96x96" }]
    },
    {
      "name": "Portfolio",
      "short_name": "Portfolio",
      "description": "View your trading portfolio",
      "url": "/trading/portfolio",
      "icons": [{ "src": "/icons/portfolio-96x96.png", "sizes": "96x96" }]
    }
  ],
  "screenshots": [
    {
      "src": "/screenshots/desktop-dashboard.png",
      "sizes": "1280x720",
      "type": "image/png",
      "form_factor": "wide",
      "label": "Dashboard on desktop"
    },
    {
      "src": "/screenshots/mobile-signals.png",
      "sizes": "390x844",
      "type": "image/png",
      "form_factor": "narrow",
      "label": "Live signals on mobile"
    }
  ]
}

// Service Worker for Offline Support
// sw.js
self.addEventListener('install', (event) => {
  console.log('Service Worker: Installed');
  
  event.waitUntil(
    caches.open('treum-static-v1')
      .then((cache) => {
        return cache.addAll([
          '/',
          '/offline',
          '/icons/icon-192x192.png',
          '/css/app.css',
          '/js/app.js'
        ]);
      })
  );
});

self.addEventListener('fetch', (event) => {
  // Handle API requests with network-first strategy
  if (event.request.url.includes('/api/')) {
    event.respondWith(
      fetch(event.request)
        .then((response) => {
          // Clone the response
          const responseClone = response.clone();
          
          // Store in cache if successful
          if (response.status === 200) {
            caches.open('treum-api-cache')
              .then((cache) => {
                cache.put(event.request, responseClone);
              });
          }
          
          return response;
        })
        .catch(() => {
          // Return cached version if network fails
          return caches.match(event.request);
        })
    );
    return;
  }
  
  // Handle static assets with cache-first strategy
  event.respondWith(
    caches.match(event.request)
      .then((response) => {
        return response || fetch(event.request);
      })
      .catch(() => {
        // Return offline page for navigation requests
        if (event.request.mode === 'navigate') {
          return caches.match('/offline');
        }
      })
  );
});

// Background Sync for Offline Actions
self.addEventListener('sync', (event) => {
  if (event.tag === 'background-sync') {
    event.waitUntil(doBackgroundSync());
  }
});

async function doBackgroundSync() {
  try {
    // Sync pending actions when connection is restored
    const pendingActions = await getStoredActions();
    
    for (const action of pendingActions) {
      try {
        await syncAction(action);
        await removeStoredAction(action.id);
      } catch (error) {
        console.log('Failed to sync action:', error);
      }
    }
  } catch (error) {
    console.log('Background sync failed:', error);
  }
}

// Push Notifications
self.addEventListener('push', (event) => {
  const options = {
    body: event.data ? event.data.text() : 'New signal available!',
    icon: '/icons/icon-192x192.png',
    badge: '/icons/badge-72x72.png',
    vibrate: [100, 50, 100],
    data: {
      dateOfArrival: Date.now(),
      primaryKey: '2'
    },
    actions: [
      {
        action: 'explore',
        title: 'View Signal',
        icon: '/icons/checkmark.png'
      },
      {
        action: 'close',
        title: 'Close',
        icon: '/icons/xmark.png'
      }
    ]
  };
  
  event.waitUntil(
    self.registration.showNotification('TREUM Alert', options)
  );
});

// Notification Click Handler
self.addEventListener('notificationclick', (event) => {
  event.notification.close();
  
  if (event.action === 'explore') {
    event.waitUntil(
      clients.openWindow('/signals')
    );
  }
});
```

---

## 2. Brownfield Architecture Strategy

### 2.1 Legacy System Integration Patterns

**Current Legacy Systems Assessment**:

```yaml
Legacy System Inventory:
  Trading Platform (Legacy):
    Technology: Java Spring Boot 2.x + Oracle DB
    Age: 5 years
    Users: 50K+ active
    Data Volume: 2TB+ transaction history
    Criticality: HIGH
    Migration Priority: Phase 2
    
  Customer Support System:
    Technology: PHP Laravel + MySQL
    Age: 3 years
    Users: 200 support agents
    Data Volume: 500GB+ tickets/logs
    Criticality: MEDIUM
    Migration Priority: Phase 3
    
  Content Management System:
    Technology: WordPress + MySQL
    Age: 2 years
    Users: Content team (20 users)
    Data Volume: 100GB+ media/content
    Criticality: LOW
    Migration Priority: Phase 4
    
  Payment Gateway Legacy:
    Technology: .NET Framework 4.8 + SQL Server
    Age: 4 years
    Users: All customers (payment processing)
    Data Volume: 1TB+ financial transactions
    Criticality: CRITICAL
    Migration Priority: Phase 1
```

**Strangler Fig Pattern Implementation**:

```typescript
// API Gateway with Strangler Fig Pattern
interface LegacyRoute {
  path: string;
  legacyService: string;
  newService?: string;
  migrationStatus: 'legacy' | 'dual' | 'migrated';
  trafficSplit?: number; // Percentage to new service
}

class StranglerFigRouter {
  private routes: LegacyRoute[] = [
    {
      path: '/api/payments/*',
      legacyService: 'legacy-payment-service',
      newService: 'modern-payment-service',
      migrationStatus: 'dual',
      trafficSplit: 20 // 20% to new service
    },
    {
      path: '/api/trading/*',
      legacyService: 'legacy-trading-service',
      migrationStatus: 'legacy'
    },
    {
      path: '/api/support/*',
      legacyService: 'legacy-support-service',
      newService: 'modern-support-service',
      migrationStatus: 'dual',
      trafficSplit: 10
    }
  ];

  async routeRequest(request: Request): Promise<Response> {
    const route = this.findMatchingRoute(request.url);
    
    if (!route) {
      return new Response('Not Found', { status: 404 });
    }

    switch (route.migrationStatus) {
      case 'legacy':
        return await this.callLegacyService(route, request);
        
      case 'migrated':
        return await this.callNewService(route, request);
        
      case 'dual':
        return await this.handleDualMode(route, request);
        
      default:
        throw new Error(`Unknown migration status: ${route.migrationStatus}`);
    }
  }

  private async handleDualMode(route: LegacyRoute, request: Request): Promise<Response> {
    const useNewService = Math.random() < (route.trafficSplit! / 100);
    const userId = this.extractUserId(request);
    
    // Feature flag check for specific users
    const featureFlags = await this.getFeatureFlags(userId);
    const shouldUseNewService = useNewService || featureFlags.includes('new-payment-flow');

    if (shouldUseNewService && route.newService) {
      try {
        // Try new service first
        const response = await this.callNewService(route, request);
        
        // Log success metrics
        await this.logMigrationMetrics({
          route: route.path,
          service: 'new',
          success: true,
          responseTime: response.headers.get('x-response-time')
        });
        
        return response;
      } catch (error) {
        // Fallback to legacy service
        console.warn('New service failed, falling back to legacy:', error);
        
        await this.logMigrationMetrics({
          route: route.path,
          service: 'new',
          success: false,
          error: error.message
        });
        
        return await this.callLegacyService(route, request);
      }
    } else {
      // Use legacy service
      const response = await this.callLegacyService(route, request);
      
      await this.logMigrationMetrics({
        route: route.path,
        service: 'legacy',
        success: true,
        responseTime: response.headers.get('x-response-time')
      });
      
      return response;
    }
  }

  private async callNewService(route: LegacyRoute, request: Request): Promise<Response> {
    const newRequest = this.transformRequest(request, 'new');
    const response = await fetch(`${route.newService}${newRequest.url}`, newRequest);
    return this.transformResponse(response, 'new');
  }

  private async callLegacyService(route: LegacyRoute, request: Request): Promise<Response> {
    const legacyRequest = this.transformRequest(request, 'legacy');
    const response = await fetch(`${route.legacyService}${legacyRequest.url}`, legacyRequest);
    return this.transformResponse(response, 'legacy');
  }
}

// Data Transformation Layer
class DataTransformationService {
  // Transform legacy data structures to new format
  transformLegacyUser(legacyUser: LegacyUser): ModernUser {
    return {
      id: legacyUser.user_id,
      email: legacyUser.email_address,
      profile: {
        firstName: legacyUser.first_name,
        lastName: legacyUser.last_name,
        phone: legacyUser.phone_number,
        dateOfBirth: new Date(legacyUser.dob),
      },
      kyc: {
        status: this.mapKycStatus(legacyUser.kyc_flag),
        documents: this.transformDocuments(legacyUser.documents),
        verifiedAt: legacyUser.kyc_verified_date ? new Date(legacyUser.kyc_verified_date) : null,
      },
      subscription: {
        tier: this.mapSubscriptionTier(legacyUser.subscription_type),
        status: legacyUser.subscription_active ? 'active' : 'inactive',
        expiresAt: new Date(legacyUser.subscription_expiry),
      },
      createdAt: new Date(legacyUser.created_timestamp),
      updatedAt: new Date(legacyUser.last_updated),
    };
  }

  // Transform modern data structures to legacy format
  transformModernUser(modernUser: ModernUser): LegacyUser {
    return {
      user_id: modernUser.id,
      email_address: modernUser.email,
      first_name: modernUser.profile.firstName,
      last_name: modernUser.profile.lastName,
      phone_number: modernUser.profile.phone,
      dob: modernUser.profile.dateOfBirth.toISOString(),
      kyc_flag: this.mapModernKycStatus(modernUser.kyc.status),
      kyc_verified_date: modernUser.kyc.verifiedAt?.toISOString(),
      subscription_type: this.mapModernSubscriptionTier(modernUser.subscription.tier),
      subscription_active: modernUser.subscription.status === 'active',
      subscription_expiry: modernUser.subscription.expiresAt.toISOString(),
      created_timestamp: modernUser.createdAt.toISOString(),
      last_updated: modernUser.updatedAt.toISOString(),
    };
  }

  private mapKycStatus(legacyStatus: string): KycStatus {
    const statusMap: Record<string, KycStatus> = {
      'Y': 'verified',
      'N': 'pending',
      'R': 'rejected',
      'E': 'expired',
    };
    return statusMap[legacyStatus] || 'pending';
  }

  private mapSubscriptionTier(legacyTier: string): SubscriptionTier {
    const tierMap: Record<string, SubscriptionTier> = {
      'FREE': 'free',
      'BASIC': 'basic',
      'PREMIUM': 'premium',
      'ENTERPRISE': 'enterprise',
    };
    return tierMap[legacyTier] || 'free';
  }
}
```

### 2.2 Event-Driven Integration Architecture

**Event Bus Implementation**:

```typescript
// Event-Driven Integration with Legacy Systems
interface IntegrationEvent {
  id: string;
  type: string;
  source: 'legacy' | 'modern';
  target: 'legacy' | 'modern';
  payload: any;
  timestamp: Date;
  version: string;
  metadata?: Record<string, any>;
}

class EventDrivenIntegration {
  private eventBus: EventBus;
  private eventStore: EventStore;
  private transformationService: DataTransformationService;

  constructor() {
    this.eventBus = new KafkaEventBus();
    this.eventStore = new PostgreSQLEventStore();
    this.transformationService = new DataTransformationService();
  }

  // Handle events from legacy systems
  async handleLegacyEvent(event: IntegrationEvent): Promise<void> {
    try {
      // Store event for audit trail
      await this.eventStore.store(event);

      // Transform legacy event to modern format
      const modernEvent = await this.transformLegacyEvent(event);

      // Publish to modern system event bus
      await this.eventBus.publish('modern.system', modernEvent);

      // Update integration metrics
      await this.updateIntegrationMetrics(event, 'success');

    } catch (error) {
      console.error('Failed to handle legacy event:', error);
      
      // Store failure for retry
      await this.eventStore.storeFailed(event, error);
      
      // Update integration metrics
      await this.updateIntegrationMetrics(event, 'failure');
      
      // Trigger alert for critical events
      if (this.isCriticalEvent(event)) {
        await this.triggerAlert(event, error);
      }
    }
  }

  // Handle events from modern systems to legacy
  async handleModernEvent(event: IntegrationEvent): Promise<void> {
    try {
      // Check if legacy system can handle this event type
      if (!this.isLegacyCompatible(event.type)) {
        console.log('Event not compatible with legacy system:', event.type);
        return;
      }

      // Transform modern event to legacy format
      const legacyEvent = await this.transformModernEvent(event);

      // Send to legacy system via REST API or message queue
      await this.sendToLegacySystem(legacyEvent);

      await this.updateIntegrationMetrics(event, 'success');

    } catch (error) {
      console.error('Failed to send event to legacy system:', error);
      
      // Store for retry with exponential backoff
      await this.scheduleRetry(event, error);
    }
  }

  private async transformLegacyEvent(event: IntegrationEvent): Promise<IntegrationEvent> {
    let transformedPayload: any;

    switch (event.type) {
      case 'USER_CREATED':
        transformedPayload = this.transformationService.transformLegacyUser(event.payload);
        break;
        
      case 'PAYMENT_COMPLETED':
        transformedPayload = this.transformLegacyPayment(event.payload);
        break;
        
      case 'TRADE_EXECUTED':
        transformedPayload = this.transformLegacyTrade(event.payload);
        break;
        
      default:
        // Generic transformation for unknown event types
        transformedPayload = await this.genericTransformation(event.payload);
    }

    return {
      ...event,
      id: generateUUID(),
      source: 'legacy',
      target: 'modern',
      payload: transformedPayload,
      version: '2.0',
      metadata: {
        ...event.metadata,
        transformedAt: new Date(),
        originalEventId: event.id,
      }
    };
  }

  // Sync data between legacy and modern systems
  async syncData(syncType: 'full' | 'incremental', entityType: string): Promise<void> {
    const startTime = Date.now();
    
    try {
      switch (syncType) {
        case 'full':
          await this.performFullSync(entityType);
          break;
        case 'incremental':
          await this.performIncrementalSync(entityType);
          break;
      }

      await this.logSyncCompletion(syncType, entityType, Date.now() - startTime);

    } catch (error) {
      await this.logSyncFailure(syncType, entityType, error);
      throw error;
    }
  }

  private async performIncrementalSync(entityType: string): Promise<void> {
    // Get last sync timestamp
    const lastSync = await this.getLastSyncTimestamp(entityType);
    
    // Fetch changes from legacy system since last sync
    const legacyChanges = await this.fetchLegacyChanges(entityType, lastSync);
    
    // Fetch changes from modern system since last sync
    const modernChanges = await this.fetchModernChanges(entityType, lastSync);
    
    // Resolve conflicts using business rules
    const resolvedChanges = await this.resolveConflicts(legacyChanges, modernChanges);
    
    // Apply changes to both systems
    await Promise.all([
      this.applyChangesToLegacy(resolvedChanges.legacy),
      this.applyChangesToModern(resolvedChanges.modern)
    ]);
    
    // Update sync timestamp
    await this.updateLastSyncTimestamp(entityType, new Date());
  }

  // Conflict Resolution Strategy
  private async resolveConflicts(
    legacyChanges: any[], 
    modernChanges: any[]
  ): Promise<{ legacy: any[], modern: any[] }> {
    const conflictResolver = new ConflictResolver();
    
    return conflictResolver.resolve({
      legacy: legacyChanges,
      modern: modernChanges,
      rules: [
        // Modern system wins for user profile changes
        { entity: 'user', field: 'profile', priority: 'modern' },
        
        // Legacy system wins for trading data
        { entity: 'trade', field: '*', priority: 'legacy' },
        
        // Last writer wins for non-critical data
        { entity: '*', field: '*', priority: 'timestamp' },
      ]
    });
  }
}

// Event Sourcing for Legacy Integration
class IntegrationEventStore {
  private db: PostgreSQLConnection;

  async storeEvent(event: IntegrationEvent): Promise<void> {
    await this.db.query(`
      INSERT INTO integration_events (
        id, type, source, target, payload, timestamp, version, metadata
      ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
    `, [
      event.id,
      event.type,
      event.source,
      event.target,
      JSON.stringify(event.payload),
      event.timestamp,
      event.version,
      JSON.stringify(event.metadata || {})
    ]);
  }

  async getEventHistory(entityId: string, entityType: string): Promise<IntegrationEvent[]> {
    const result = await this.db.query(`
      SELECT * FROM integration_events 
      WHERE payload->>'id' = $1 AND type LIKE $2
      ORDER BY timestamp ASC
    `, [entityId, `${entityType}_%`]);

    return result.rows.map(row => this.mapRowToEvent(row));
  }

  async replayEvents(fromTimestamp: Date, toTimestamp: Date): Promise<void> {
    const events = await this.db.query(`
      SELECT * FROM integration_events 
      WHERE timestamp BETWEEN $1 AND $2
      ORDER BY timestamp ASC
    `, [fromTimestamp, toTimestamp]);

    for (const eventRow of events.rows) {
      const event = this.mapRowToEvent(eventRow);
      await this.replayEvent(event);
    }
  }

  private async replayEvent(event: IntegrationEvent): Promise<void> {
    // Replay event to reconstruct state
    console.log(`Replaying event: ${event.type} at ${event.timestamp}`);
    
    // Apply event to appropriate handler
    const handler = this.getEventHandler(event.type);
    if (handler) {
      await handler.handle(event);
    }
  }
}
```

### 2.3 Database Synchronization Strategy

**Multi-Database Sync Implementation**:

```typescript
// Database Synchronization Service
class DatabaseSyncService {
  private legacyDB: OracleConnection;
  private modernDB: PostgreSQLConnection;
  private syncLog: SyncLogService;
  private conflictResolver: ConflictResolver;

  constructor() {
    this.legacyDB = new OracleConnection(config.legacy.database);
    this.modernDB = new PostgreSQLConnection(config.modern.database);
    this.syncLog = new SyncLogService();
    this.conflictResolver = new ConflictResolver();
  }

  // Bi-directional sync with conflict resolution
  async syncBidirectional(entityType: string): Promise<SyncResult> {
    const syncSession = await this.syncLog.startSession(entityType);
    
    try {
      // 1. Fetch changes from both systems
      const [legacyChanges, modernChanges] = await Promise.all([
        this.fetchLegacyChanges(entityType, syncSession.lastSync),
        this.fetchModernChanges(entityType, syncSession.lastSync)
      ]);

      // 2. Detect conflicts
      const conflicts = this.detectConflicts(legacyChanges, modernChanges);
      
      // 3. Resolve conflicts using business rules
      const resolutions = await this.conflictResolver.resolveConflicts(conflicts);
      
      // 4. Apply changes to both systems
      const syncResults = await this.applyChanges(legacyChanges, modernChanges, resolutions);
      
      // 5. Update sync log
      await this.syncLog.completeSession(syncSession.id, syncResults);
      
      return {
        success: true,
        recordsSynced: syncResults.totalRecords,
        conflictsResolved: conflicts.length,
        duration: Date.now() - syncSession.startTime,
      };

    } catch (error) {
      await this.syncLog.failSession(syncSession.id, error);
      throw error;
    }
  }

  private async fetchLegacyChanges(entityType: string, since: Date): Promise<ChangeRecord[]> {
    const query = this.buildLegacyChangeQuery(entityType, since);
    const result = await this.legacyDB.query(query);
    
    return result.rows.map(row => ({
      id: row.ID,
      entityType,
      operation: this.mapLegacyOperation(row.OPERATION),
      data: this.transformLegacyData(row),
      timestamp: new Date(row.LAST_MODIFIED),
      version: row.VERSION || 1,
      source: 'legacy'
    }));
  }

  private async fetchModernChanges(entityType: string, since: Date): Promise<ChangeRecord[]> {
    const result = await this.modernDB.query(`
      SELECT * FROM change_log 
      WHERE entity_type = $1 AND timestamp > $2
      ORDER BY timestamp ASC
    `, [entityType, since]);
    
    return result.rows.map(row => ({
      id: row.entity_id,
      entityType: row.entity_type,
      operation: row.operation,
      data: JSON.parse(row.data),
      timestamp: new Date(row.timestamp),
      version: row.version,
      source: 'modern'
    }));
  }

  private detectConflicts(legacyChanges: ChangeRecord[], modernChanges: ChangeRecord[]): Conflict[] {
    const conflicts: Conflict[] = [];
    
    // Group changes by entity ID
    const legacyByEntity = groupBy(legacyChanges, 'id');
    const modernByEntity = groupBy(modernChanges, 'id');
    
    // Find entities that were modified in both systems
    const commonEntities = intersection(
      Object.keys(legacyByEntity),
      Object.keys(modernByEntity)
    );
    
    for (const entityId of commonEntities) {
      const legacyChange = legacyByEntity[entityId][0]; // Latest change
      const modernChange = modernByEntity[entityId][0]; // Latest change
      
      // Check if changes conflict
      if (this.isConflicting(legacyChange, modernChange)) {
        conflicts.push({
          entityId,
          entityType: legacyChange.entityType,
          legacyChange,
          modernChange,
          conflictType: this.determineConflictType(legacyChange, modernChange),
        });
      }
    }
    
    return conflicts;
  }

  private async applyChanges(
    legacyChanges: ChangeRecord[],
    modernChanges: ChangeRecord[],
    resolutions: ConflictResolution[]
  ): Promise<SyncResults> {
    const results = {
      legacyToModern: 0,
      modernToLegacy: 0,
      conflictsResolved: 0,
      totalRecords: 0,
    };

    // Apply legacy changes to modern system
    for (const change of legacyChanges) {
      const resolution = resolutions.find(r => r.entityId === change.id);
      
      if (!resolution || resolution.winner === 'legacy') {
        await this.applyChangeToModern(change);
        results.legacyToModern++;
      }
    }

    // Apply modern changes to legacy system
    for (const change of modernChanges) {
      const resolution = resolutions.find(r => r.entityId === change.id);
      
      if (!resolution || resolution.winner === 'modern') {
        await this.applyChangeToLegacy(change);
        results.modernToLegacy++;
      }
    }

    results.conflictsResolved = resolutions.length;
    results.totalRecords = results.legacyToModern + results.modernToLegacy;

    return results;
  }

  // Change Data Capture (CDC) for real-time sync
  async setupCDC(): Promise<void> {
    // Set up Oracle CDC for legacy database
    await this.setupOracleCDC();
    
    // Set up PostgreSQL CDC for modern database
    await this.setupPostgreSQLCDC();
  }

  private async setupOracleCDC(): Promise<void> {
    // Enable Oracle LogMiner for change capture
    await this.legacyDB.execute(`
      BEGIN
        DBMS_LOGMNR.START_LOGMNR(
          STARTTIME => SYSDATE - 1,
          ENDTIME => SYSDATE,
          OPTIONS => DBMS_LOGMNR.DICT_FROM_ONLINE_CATALOG
        );
      END;
    `);

    // Set up trigger-based CDC for critical tables
    const tables = ['USERS', 'TRANSACTIONS', 'TRADES'];
    
    for (const table of tables) {
      await this.createCDCTrigger(table);
    }
  }

  private async createCDCTrigger(tableName: string): Promise<void> {
    await this.legacyDB.execute(`
      CREATE OR REPLACE TRIGGER ${tableName}_CDC_TRIGGER
      AFTER INSERT OR UPDATE OR DELETE ON ${tableName}
      FOR EACH ROW
      BEGIN
        INSERT INTO CDC_LOG (
          table_name,
          operation,
          primary_key,
          old_values,
          new_values,
          timestamp
        ) VALUES (
          '${tableName}',
          CASE 
            WHEN INSERTING THEN 'INSERT'
            WHEN UPDATING THEN 'UPDATE'
            WHEN DELETING THEN 'DELETE'
          END,
          CASE 
            WHEN INSERTING OR UPDATING THEN :NEW.ID
            WHEN DELETING THEN :OLD.ID
          END,
          CASE 
            WHEN UPDATING OR DELETING THEN JSON_OBJECT(${this.buildOldValuesJSON(tableName)})
          END,
          CASE 
            WHEN INSERTING OR UPDATING THEN JSON_OBJECT(${this.buildNewValuesJSON(tableName)})
          END,
          SYSTIMESTAMP
        );
      END;
    `);
  }

  private async setupPostgreSQLCDC(): Promise<void> {
    // Enable logical replication
    await this.modernDB.query(`
      CREATE PUBLICATION treum_publication FOR ALL TABLES;
    `);

    // Create replication slot
    await this.modernDB.query(`
      SELECT pg_create_logical_replication_slot('treum_slot', 'pgoutput');
    `);

    // Set up Debezium connector for Kafka
    const debeziumConfig = {
      "name": "treum-postgres-connector",
      "config": {
        "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
        "database.hostname": process.env.POSTGRES_HOST,
        "database.port": "5432",
        "database.user": process.env.POSTGRES_USER,
        "database.password": process.env.POSTGRES_PASSWORD,
        "database.dbname": "treum",
        "database.server.name": "treum-postgres",
        "slot.name": "treum_slot",
        "publication.name": "treum_publication",
        "table.whitelist": "public.users,public.transactions,public.trades",
        "transforms": "unwrap",
        "transforms.unwrap.type": "io.debezium.transforms.ExtractNewRecordState"
      }
    };

    // Register Debezium connector
    await this.registerDebeziumConnector(debeziumConfig);
  }
}

// Data Validation Service
class DataValidationService {
  async validateSyncIntegrity(entityType: string): Promise<ValidationResult> {
    const [legacyCount, modernCount] = await Promise.all([
      this.countLegacyRecords(entityType),
      this.countModernRecords(entityType)
    ]);

    // Sample records for deep validation
    const sampleSize = Math.min(1000, Math.floor(legacyCount * 0.01));
    const sampleRecords = await this.getSampleRecords(entityType, sampleSize);

    const mismatches: DataMismatch[] = [];
    
    for (const record of sampleRecords) {
      const legacyData = await this.fetchLegacyRecord(entityType, record.id);
      const modernData = await this.fetchModernRecord(entityType, record.id);
      
      if (legacyData && modernData) {
        const comparison = this.compareRecords(legacyData, modernData);
        if (!comparison.matches) {
          mismatches.push({
            id: record.id,
            entityType,
            differences: comparison.differences,
          });
        }
      }
    }

    return {
      entityType,
      legacyCount,
      modernCount,
      countMismatch: legacyCount !== modernCount,
      dataMismatches: mismatches,
      accuracyRate: ((sampleSize - mismatches.length) / sampleSize) * 100,
      validatedAt: new Date(),
    };
  }

  private compareRecords(legacy: any, modern: any): RecordComparison {
    const differences: FieldDifference[] = [];
    const transformedLegacy = this.normalizeRecord(legacy);
    const transformedModern = this.normalizeRecord(modern);

    for (const field in transformedLegacy) {
      if (transformedLegacy[field] !== transformedModern[field]) {
        differences.push({
          field,
          legacyValue: transformedLegacy[field],
          modernValue: transformedModern[field],
        });
      }
    }

    return {
      matches: differences.length === 0,
      differences,
    };
  }
}
```

---

## 3. Integration Architecture

### 3.1 API Gateway & Service Mesh Integration

**Kong Gateway with Istio Service Mesh**:

```yaml
# Kong Gateway Configuration for Micro-frontend Integration
apiVersion: v1
kind: ConfigMap
metadata:
  name: kong-integration-config
data:
  kong.yml: |
    _format_version: "3.0"
    
    # Frontend Route Definitions
    services:
    - name: shell-app
      url: http://shell-app.frontend.svc.cluster.local:3000
      routes:
      - name: shell-routes
        paths: ["/", "/dashboard", "/profile"]
        
    - name: education-frontend
      url: http://education-frontend.frontend.svc.cluster.local:3001
      routes:
      - name: education-routes
        paths: ["/education"]
        
    - name: signals-frontend  
      url: http://signals-frontend.frontend.svc.cluster.local:3002
      routes:
      - name: signals-routes
        paths: ["/signals"]
        
    - name: trading-frontend
      url: http://trading-frontend.frontend.svc.cluster.local:3003
      routes:
      - name: trading-routes
        paths: ["/trading"]

    # Backend API Routes  
    - name: user-api
      url: http://user-service.backend.svc.cluster.local:8080
      plugins:
      - name: jwt
      - name: rate-limiting
        config:
          minute: 1000
      routes:
      - name: user-api-routes
        paths: ["/api/v1/users"]
        
    - name: education-api
      url: http://education-service.backend.svc.cluster.local:8081
      routes:
      - name: education-api-routes
        paths: ["/api/v1/courses", "/api/v1/enrollments"]
        
    - name: signals-api
      url: http://signals-service.backend.svc.cluster.local:8082
      plugins:
      - name: subscription-validator
      routes:
      - name: signals-api-routes
        paths: ["/api/v1/signals"]
        
    # Legacy System Integration
    - name: legacy-trading-api
      url: http://legacy-trading.legacy.svc.cluster.local:8080
      plugins:
      - name: request-transformer
        config:
          add:
            headers:
            - "X-Legacy-Source: kong-gateway"
      - name: response-transformer
        config:
          add:
            headers:
            - "X-Modernized: true"
      routes:
      - name: legacy-trading-routes
        paths: ["/api/legacy/trading"]

# Istio Service Mesh Configuration
---
apiVersion: networking.istio.io/v1beta1
kind: Gateway
metadata:
  name: treum-gateway
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 443
      name: https
      protocol: HTTPS
    tls:
      mode: SIMPLE
      credentialName: treum-tls-secret
    hosts:
    - "*.treum.in"
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - "*.treum.in"
    redirect:
      httpsRedirect: true

---
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: treum-routing
spec:
  hosts:
  - "*.treum.in"
  gateways:
  - treum-gateway
  http:
  # Frontend routes
  - match:
    - uri:
        prefix: "/education"
    route:
    - destination:
        host: education-frontend.frontend.svc.cluster.local
        port:
          number: 3001
    fault:
      delay:
        percentage:
          value: 0.1
        fixedDelay: 5s
      abort:
        percentage:
          value: 0.01
        httpStatus: 500
        
  - match:
    - uri:
        prefix: "/signals"
    route:
    - destination:
        host: signals-frontend.frontend.svc.cluster.local
        port:
          number: 3002
    headers:
      request:
        add:
          x-request-source: "istio-gateway"
          
  # API routes with load balancing
  - match:
    - uri:
        prefix: "/api/v1/signals"
    route:
    - destination:
        host: signals-service.backend.svc.cluster.local
        port:
          number: 8082
      weight: 90
    - destination:
        host: signals-service-canary.backend.svc.cluster.local
        port:
          number: 8082
      weight: 10
    timeout: 30s
    retries:
      attempts: 3
      perTryTimeout: 10s
      
  # Legacy system integration with circuit breaker
  - match:
    - uri:
        prefix: "/api/legacy"
    route:
    - destination:
        host: legacy-trading.legacy.svc.cluster.local
        port:
          number: 8080
    fault:
      abort:
        percentage:
          value: 0.5
        httpStatus: 503
    timeout: 60s
    retries:
      attempts: 2
      perTryTimeout: 30s

---
# Destination Rules for Load Balancing
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: signals-service-dr
spec:
  host: signals-service.backend.svc.cluster.local
  trafficPolicy:
    loadBalancer:
      simple: LEAST_CONN
    connectionPool:
      tcp:
        maxConnections: 100
      http:
        http1MaxPendingRequests: 10
        maxRequestsPerConnection: 2
    circuitBreaker:
      consecutiveErrors: 3
      interval: 30s
      baseEjectionTime: 30s
      maxEjectionPercent: 50
  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2
```

### 3.2 Event Bus Architecture

**Centralized Event Bus with Kafka**:

```typescript
// Unified Event Bus for Frontend, Backend, and Legacy Integration
interface UnifiedEvent {
  id: string;
  type: string;
  source: 'frontend' | 'backend' | 'legacy';
  target?: string[];
  payload: any;
  timestamp: Date;
  correlationId?: string;
  userId?: string;
  sessionId?: string;
  metadata?: Record<string, any>;
}

class UnifiedEventBus {
  private kafka: KafkaClient;
  private redis: RedisClient;
  private eventHandlers: Map<string, EventHandler[]>;
  private eventStore: EventStore;

  constructor() {
    this.kafka = new KafkaClient();
    this.redis = new RedisClient();
    this.eventHandlers = new Map();
    this.eventStore = new EventStore();
  }

  // Publish events to the unified bus
  async publish(event: UnifiedEvent): Promise<void> {
    try {
      // Add correlation tracking
      if (!event.correlationId) {
        event.correlationId = generateCorrelationId();
      }

      // Store event for audit and replay
      await this.eventStore.store(event);

      // Determine target topics based on event type and source
      const topics = this.determineTargetTopics(event);

      // Publish to Kafka topics
      for (const topic of topics) {
        await this.kafka.publish(topic, event);
      }

      // Update real-time subscribers via Redis
      await this.notifyRealtimeSubscribers(event);

      // Log event metrics
      await this.updateEventMetrics(event);

    } catch (error) {
      console.error('Failed to publish event:', error);
      await this.handlePublishError(event, error);
    }
  }

  // Subscribe to events with filtering
  async subscribe(
    eventTypes: string[],
    handler: EventHandler,
    options: SubscriptionOptions = {}
  ): Promise<() => void> {
    const subscription = {
      id: generateUUID(),
      eventTypes,
      handler,
      options,
      createdAt: new Date(),
    };

    // Register handler for each event type
    for (const eventType of eventTypes) {
      if (!this.eventHandlers.has(eventType)) {
        this.eventHandlers.set(eventType, []);
      }
      this.eventHandlers.get(eventType)!.push(handler);
    }

    // Subscribe to Kafka topics
    const topics = eventTypes.map(type => this.getTopicForEventType(type));
    await this.kafka.subscribe(topics, (message) => {
      this.handleIncomingEvent(message, subscription);
    });

    // Return unsubscribe function
    return () => this.unsubscribe(subscription);
  }

  private determineTargetTopics(event: UnifiedEvent): string[] {
    const topics: string[] = [];

    switch (event.source) {
      case 'frontend':
        topics.push('frontend-events');
        // Route to backend if needed
        if (this.requiresBackendProcessing(event.type)) {
          topics.push('backend-events');
        }
        // Route to legacy if needed
        if (this.requiresLegacySync(event.type)) {
          topics.push('legacy-events');
        }
        break;

      case 'backend':
        topics.push('backend-events');
        // Route to frontend for UI updates
        if (this.requiresFrontendUpdate(event.type)) {
          topics.push('frontend-events');
        }
        // Route to legacy for synchronization
        if (this.requiresLegacySync(event.type)) {
          topics.push('legacy-events');
        }
        break;

      case 'legacy':
        topics.push('legacy-events');
        // Always route legacy events to backend for processing
        topics.push('backend-events');
        // Route to frontend if it affects UI
        if (this.affectsUI(event.type)) {
          topics.push('frontend-events');
        }
        break;
    }

    return topics;
  }

  // Real-time event distribution to connected clients
  private async notifyRealtimeSubscribers(event: UnifiedEvent): Promise<void> {
    // Get active WebSocket connections for the user
    const connectionIds = await this.redis.get(`user:${event.userId}:connections`);
    
    if (connectionIds) {
      const connections = JSON.parse(connectionIds);
      
      for (const connectionId of connections) {
        try {
          await this.sendToWebSocket(connectionId, event);
        } catch (error) {
          console.warn(`Failed to send event to connection ${connectionId}:`, error);
          // Remove invalid connection
          await this.removeInvalidConnection(event.userId!, connectionId);
        }
      }
    }

    // Publish to Redis pub/sub for real-time updates
    await this.redis.publish(`events:${event.type}`, JSON.stringify(event));
  }

  // Event replay for system recovery
  async replayEvents(
    fromTimestamp: Date,
    toTimestamp: Date,
    eventTypes?: string[]
  ): Promise<void> {
    const events = await this.eventStore.getEvents({
      fromTimestamp,
      toTimestamp,
      eventTypes,
    });

    console.log(`Replaying ${events.length} events from ${fromTimestamp} to ${toTimestamp}`);

    for (const event of events) {
      try {
        await this.replayEvent(event);
      } catch (error) {
        console.error(`Failed to replay event ${event.id}:`, error);
      }
    }
  }

  private async replayEvent(event: UnifiedEvent): Promise<void> {
    // Mark as replay to avoid infinite loops
    const replayEvent = {
      ...event,
      metadata: {
        ...event.metadata,
        isReplay: true,
        originalTimestamp: event.timestamp,
        replayedAt: new Date(),
      }
    };

    // Process through handlers without publishing to Kafka
    const handlers = this.eventHandlers.get(event.type) || [];
    
    for (const handler of handlers) {
      try {
        await handler.handle(replayEvent);
      } catch (error) {
        console.error(`Handler failed during replay for event ${event.id}:`, error);
      }
    }
  }
}

// Cross-System Event Handlers
class CrossSystemEventHandler {
  private frontendSync: FrontendSyncService;
  private backendSync: BackendSyncService;
  private legacySync: LegacySyncService;

  async handleUserProfileUpdate(event: UnifiedEvent): Promise<void> {
    const { userId, profileData } = event.payload;

    switch (event.source) {
      case 'frontend':
        // Update backend database
        await this.backendSync.updateUserProfile(userId, profileData);
        
        // Sync to legacy system if needed
        if (this.requiresLegacyUserSync(userId)) {
          await this.legacySync.updateUserProfile(userId, profileData);
        }
        break;

      case 'backend':
        // Update frontend state for real-time users
        await this.frontendSync.broadcastProfileUpdate(userId, profileData);
        
        // Sync to legacy system
        await this.legacySync.updateUserProfile(userId, profileData);
        break;

      case 'legacy':
        // Update backend database
        await this.backendSync.updateUserProfile(userId, profileData);
        
        // Update frontend for active users
        await this.frontendSync.broadcastProfileUpdate(userId, profileData);
        break;
    }
  }

  async handlePaymentCompleted(event: UnifiedEvent): Promise<void> {
    const { paymentId, userId, amount, courseId } = event.payload;

    // Always process through backend for business logic
    if (event.source !== 'backend') {
      await this.backendSync.processPayment(paymentId);
    }

    // Update frontend for immediate feedback
    await this.frontendSync.updatePaymentStatus(userId, paymentId, 'completed');

    // Sync to legacy financial system
    await this.legacySync.recordPayment({
      paymentId,
      userId,
      amount,
      courseId,
      timestamp: event.timestamp,
    });

    // Trigger course enrollment if applicable
    if (courseId) {
      await this.handleCourseEnrollment({
        userId,
        courseId,
        paymentId,
        enrolledAt: event.timestamp,
      });
    }
  }

  async handleSignalGenerated(event: UnifiedEvent): Promise<void> {
    const { signal, targetUsers } = event.payload;

    // Store signal in backend
    await this.backendSync.storeSignal(signal);

    // Distribute to subscribed users via frontend
    await this.frontendSync.distributeSignal(signal, targetUsers);

    // Log to legacy system for compliance
    await this.legacySync.logSignal(signal);

    // Trigger analytics processing
    await this.triggerAnalyticsProcessing(signal);
  }

  private async triggerAnalyticsProcessing(signal: any): Promise<void> {
    // Publish analytics event
    await eventBus.publish({
      id: generateUUID(),
      type: 'SIGNAL_ANALYTICS_REQUESTED',
      source: 'backend',
      payload: { signal },
      timestamp: new Date(),
    });
  }
}

// Event-Driven Saga Pattern for Complex Workflows
class PaymentSagaOrchestrator {
  private steps: SagaStep[];
  private compensationActions: Map<string, () => Promise<void>>;

  constructor() {
    this.steps = [
      { name: 'validatePayment', handler: this.validatePayment },
      { name: 'processPayment', handler: this.processPayment },
      { name: 'enrollInCourse', handler: this.enrollInCourse },
      { name: 'updateUserProfile', handler: this.updateUserProfile },
      { name: 'sendConfirmation', handler: this.sendConfirmation },
    ];
    
    this.compensationActions = new Map([
      ['processPayment', this.refundPayment],
      ['enrollInCourse', this.cancelEnrollment],
      ['updateUserProfile', this.revertProfileUpdate],
    ]);
  }

  async executePaymentSaga(paymentData: PaymentSagaData): Promise<void> {
    const sagaId = generateUUID();
    const completedSteps: string[] = [];

    try {
      for (const step of this.steps) {
        console.log(`Executing saga step: ${step.name}`);
        
        await step.handler(paymentData, sagaId);
        completedSteps.push(step.name);

        // Publish step completion event
        await eventBus.publish({
          id: generateUUID(),
          type: 'SAGA_STEP_COMPLETED',
          source: 'backend',
          payload: {
            sagaId,
            stepName: step.name,
            paymentData,
          },
          timestamp: new Date(),
        });
      }

      // Saga completed successfully
      await this.completeSaga(sagaId, paymentData);

    } catch (error) {
      console.error(`Saga ${sagaId} failed at step:`, error);
      
      // Execute compensation actions in reverse order
      await this.compensateSaga(sagaId, completedSteps.reverse(), paymentData);
      
      throw new SagaExecutionError(sagaId, error);
    }
  }

  private async compensateSaga(
    sagaId: string,
    completedSteps: string[],
    paymentData: PaymentSagaData
  ): Promise<void> {
    for (const stepName of completedSteps) {
      const compensationAction = this.compensationActions.get(stepName);
      
      if (compensationAction) {
        try {
          await compensationAction();
          
          await eventBus.publish({
            id: generateUUID(),
            type: 'SAGA_COMPENSATION_COMPLETED',
            source: 'backend',
            payload: {
              sagaId,
              stepName,
              paymentData,
            },
            timestamp: new Date(),
          });
          
        } catch (compensationError) {
          console.error(`Compensation failed for step ${stepName}:`, compensationError);
          
          // Publish compensation failure event
          await eventBus.publish({
            id: generateUUID(),
            type: 'SAGA_COMPENSATION_FAILED',
            source: 'backend',
            payload: {
              sagaId,
              stepName,
              error: compensationError.message,
              paymentData,
            },
            timestamp: new Date(),
          });
        }
      }
    }
  }
}
```

### 3.3 Data Consistency Patterns

**Eventual Consistency with Conflict Resolution**:

```typescript
// Eventual Consistency Manager
class EventualConsistencyManager {
  private conflictResolver: ConflictResolver;
  private consistencyStore: ConsistencyStore;
  private reconciliationScheduler: ReconciliationScheduler;

  constructor() {
    this.conflictResolver = new ConflictResolver();
    this.consistencyStore = new ConsistencyStore();
    this.reconciliationScheduler = new ReconciliationScheduler();
  }

  // Vector Clock implementation for distributed consistency
  async updateWithVectorClock(
    entityId: string,
    entityType: string,
    data: any,
    sourceSystem: string
  ): Promise<void> {
    // Get current vector clock
    const currentClock = await this.consistencyStore.getVectorClock(entityId, entityType);
    
    // Increment clock for source system
    const newClock = this.incrementVectorClock(currentClock, sourceSystem);
    
    // Store update with vector clock
    await this.consistencyStore.storeUpdate({
      entityId,
      entityType,
      data,
      vectorClock: newClock,
      sourceSystem,
      timestamp: new Date(),
    });

    // Check for conflicts with concurrent updates
    const conflicts = await this.detectConflicts(entityId, entityType, newClock);
    
    if (conflicts.length > 0) {
      await this.resolveConflicts(conflicts);
    }

    // Propagate update to other systems
    await this.propagateUpdate(entityId, entityType, data, newClock);
  }

  private async detectConflicts(
    entityId: string,
    entityType: string,
    vectorClock: VectorClock
  ): Promise<Conflict[]> {
    // Get concurrent updates that are not causally related
    const concurrentUpdates = await this.consistencyStore.getConcurrentUpdates(
      entityId,
      entityType,
      vectorClock
    );

    const conflicts: Conflict[] = [];

    for (const update of concurrentUpdates) {
      if (!this.isCausallyRelated(vectorClock, update.vectorClock)) {
        conflicts.push({
          entityId,
          entityType,
          update1: { vectorClock, data: await this.getLatestData(entityId, entityType) },
          update2: update,
          conflictType: this.determineConflictType(update),
        });
      }
    }

    return conflicts;
  }

  private async resolveConflicts(conflicts: Conflict[]): Promise<void> {
    for (const conflict of conflicts) {
      const resolution = await this.conflictResolver.resolve(conflict);
      
      // Apply conflict resolution
      await this.applyResolution(conflict, resolution);
      
      // Log conflict resolution for audit
      await this.logConflictResolution(conflict, resolution);
    }
  }

  // CRDT (Conflict-free Replicated Data Types) implementation
  async updateWithCRDT(
    entityId: string,
    entityType: string,
    operation: CRDTOperation
  ): Promise<void> {
    const crdtType = this.getCRDTType(entityType);
    
    switch (crdtType) {
      case 'G-Counter':
        await this.updateGCounter(entityId, operation);
        break;
      case 'PN-Counter':
        await this.updatePNCounter(entityId, operation);
        break;
      case 'LWW-Register':
        await this.updateLWWRegister(entityId, operation);
        break;
      case 'OR-Set':
        await this.updateORSet(entityId, operation);
        break;
      default:
        throw new Error(`Unsupported CRDT type: ${crdtType}`);
    }
  }

  // Last-Writer-Wins Register for user preferences
  private async updateLWWRegister(entityId: string, operation: CRDTOperation): Promise<void> {
    const currentRegister = await this.consistencyStore.getLWWRegister(entityId);
    
    if (!currentRegister || operation.timestamp > currentRegister.timestamp) {
      await this.consistencyStore.storeLWWRegister({
        entityId,
        value: operation.value,
        timestamp: operation.timestamp,
        actor: operation.actor,
      });

      // Propagate update to other replicas
      await this.propagateCRDTUpdate(entityId, 'LWW-Register', operation);
    }
  }

  // Observed-Remove Set for user collections (watchlists, favorites)
  private async updateORSet(entityId: string, operation: CRDTOperation): Promise<void> {
    const currentSet = await this.consistencyStore.getORSet(entityId);
    
    switch (operation.type) {
      case 'add':
        const elementTag = `${operation.value}:${operation.actor}:${operation.timestamp}`;
        currentSet.elements.add(elementTag);
        currentSet.observed.add(elementTag);
        break;
        
      case 'remove':
        // Mark all observed tags for this element as removed
        const tagsToRemove = Array.from(currentSet.observed)
          .filter(tag => tag.startsWith(`${operation.value}:`));
        
        tagsToRemove.forEach(tag => {
          currentSet.removed.add(tag);
        });
        break;
    }

    await this.consistencyStore.storeORSet(entityId, currentSet);
    await this.propagateCRDTUpdate(entityId, 'OR-Set', operation);
  }

  // Reconciliation process for periodic consistency checks
  async scheduleReconciliation(): Promise<void> {
    this.reconciliationScheduler.schedule(async () => {
      console.log('Starting system-wide reconciliation...');
      
      const entityTypes = ['user', 'course', 'signal', 'transaction'];
      
      for (const entityType of entityTypes) {
        try {
          await this.reconcileEntityType(entityType);
        } catch (error) {
          console.error(`Reconciliation failed for ${entityType}:`, error);
        }
      }
    }, '0 */6 * * *'); // Every 6 hours
  }

  private async reconcileEntityType(entityType: string): Promise<void> {
    const entities = await this.getAllEntitiesOfType(entityType);
    
    for (const entity of entities) {
      // Get state from all systems
      const [frontendState, backendState, legacyState] = await Promise.all([
        this.getFrontendState(entity.id, entityType),
        this.getBackendState(entity.id, entityType),
        this.getLegacyState(entity.id, entityType),
      ]);

      // Compare states and identify discrepancies
      const discrepancies = this.compareStates([
        { source: 'frontend', state: frontendState },
        { source: 'backend', state: backendState },
        { source: 'legacy', state: legacyState },
      ]);

      if (discrepancies.length > 0) {
        // Resolve discrepancies using business rules
        const canonicalState = await this.resolveDiscrepancies(discrepancies);
        
        // Apply canonical state to all systems
        await this.applyCanonicalState(entity.id, entityType, canonicalState);
        
        // Log reconciliation action
        await this.logReconciliation(entity.id, entityType, discrepancies, canonicalState);
      }
    }
  }
}

// Distributed Transaction Manager using Saga Pattern
class DistributedTransactionManager {
  private activeSagas: Map<string, SagaInstance>;
  private sagaDefinitions: Map<string, SagaDefinition>;

  constructor() {
    this.activeSagas = new Map();
    this.sagaDefinitions = new Map();
    this.setupSagaDefinitions();
  }

  private setupSagaDefinitions(): void {
    // Course Purchase Saga
    this.sagaDefinitions.set('COURSE_PURCHASE', {
      name: 'COURSE_PURCHASE',
      steps: [
        {
          name: 'VALIDATE_USER',
          action: this.validateUser,
          compensation: this.revertUserValidation,
        },
        {
          name: 'PROCESS_PAYMENT',
          action: this.processPayment,
          compensation: this.refundPayment,
        },
        {
          name: 'ENROLL_COURSE',
          action: this.enrollInCourse,
          compensation: this.cancelEnrollment,
        },
        {
          name: 'UPDATE_LEGACY',
          action: this.updateLegacySystem,
          compensation: this.revertLegacyUpdate,
        },
        {
          name: 'SEND_NOTIFICATION',
          action: this.sendEnrollmentNotification,
          compensation: this.sendCancellationNotification,
        },
      ],
    });

    // Signal Subscription Saga
    this.sagaDefinitions.set('SIGNAL_SUBSCRIPTION', {
      name: 'SIGNAL_SUBSCRIPTION',
      steps: [
        {
          name: 'VALIDATE_SUBSCRIPTION',
          action: this.validateSubscription,
          compensation: this.revertSubscriptionValidation,
        },
        {
          name: 'UPDATE_USER_TIER',
          action: this.updateUserTier,
          compensation: this.revertUserTier,
        },
        {
          name: 'CONFIGURE_SIGNALS',
          action: this.configureSignalAccess,
          compensation: this.removeSignalAccess,
        },
        {
          name: 'SYNC_LEGACY_PERMISSIONS',
          action: this.syncLegacyPermissions,
          compensation: this.revertLegacyPermissions,
        },
      ],
    });
  }

  async executeSaga(sagaType: string, data: any): Promise<string> {
    const sagaId = generateUUID();
    const sagaDefinition = this.sagaDefinitions.get(sagaType);
    
    if (!sagaDefinition) {
      throw new Error(`Unknown saga type: ${sagaType}`);
    }

    const sagaInstance: SagaInstance = {
      id: sagaId,
      type: sagaType,
      data,
      currentStep: 0,
      completedSteps: [],
      status: 'running',
      startedAt: new Date(),
    };

    this.activeSagas.set(sagaId, sagaInstance);

    try {
      await this.executeSagaSteps(sagaInstance, sagaDefinition);
      
      sagaInstance.status = 'completed';
      sagaInstance.completedAt = new Date();
      
      return sagaId;
      
    } catch (error) {
      console.error(`Saga ${sagaId} failed:`, error);
      
      sagaInstance.status = 'failed';
      sagaInstance.error = error.message;
      
      // Execute compensating actions
      await this.compensateSaga(sagaInstance, sagaDefinition);
      
      throw new SagaExecutionError(sagaId, error);
    } finally {
      // Clean up completed saga after retention period
      setTimeout(() => {
        this.activeSagas.delete(sagaId);
      }, 24 * 60 * 60 * 1000); // 24 hours
    }
  }

  private async executeSagaSteps(
    sagaInstance: SagaInstance,
    sagaDefinition: SagaDefinition
  ): Promise<void> {
    for (let i = sagaInstance.currentStep; i < sagaDefinition.steps.length; i++) {
      const step = sagaDefinition.steps[i];
      
      console.log(`Executing saga ${sagaInstance.id} step: ${step.name}`);
      
      try {
        await step.action(sagaInstance.data, sagaInstance.id);
        
        sagaInstance.completedSteps.push(step.name);
        sagaInstance.currentStep = i + 1;
        
        // Publish step completion event
        await eventBus.publish({
          id: generateUUID(),
          type: 'SAGA_STEP_COMPLETED',
          source: 'backend',
          payload: {
            sagaId: sagaInstance.id,
            sagaType: sagaInstance.type,
            stepName: step.name,
            stepIndex: i,
          },
          timestamp: new Date(),
        });
        
      } catch (stepError) {
        console.error(`Saga ${sagaInstance.id} step ${step.name} failed:`, stepError);
        
        // Publish step failure event
        await eventBus.publish({
          id: generateUUID(),
          type: 'SAGA_STEP_FAILED',
          source: 'backend',
          payload: {
            sagaId: sagaInstance.id,
            sagaType: sagaInstance.type,
            stepName: step.name,
            stepIndex: i,
            error: stepError.message,
          },
          timestamp: new Date(),
        });
        
        throw stepError;
      }
    }
  }

  private async compensateSaga(
    sagaInstance: SagaInstance,
    sagaDefinition: SagaDefinition
  ): Promise<void> {
    console.log(`Starting compensation for saga ${sagaInstance.id}`);
    
    // Execute compensation actions in reverse order
    const completedStepNames = sagaInstance.completedSteps.slice().reverse();
    
    for (const stepName of completedStepNames) {
      const step = sagaDefinition.steps.find(s => s.name === stepName);
      
      if (step && step.compensation) {
        try {
          await step.compensation(sagaInstance.data, sagaInstance.id);
          
          await eventBus.publish({
            id: generateUUID(),
            type: 'SAGA_COMPENSATION_COMPLETED',
            source: 'backend',
            payload: {
              sagaId: sagaInstance.id,
              stepName,
            },
            timestamp: new Date(),
          });
          
        } catch (compensationError) {
          console.error(`Compensation failed for step ${stepName}:`, compensationError);
          
          await eventBus.publish({
            id: generateUUID(),
            type: 'SAGA_COMPENSATION_FAILED',
            source: 'backend',
            payload: {
              sagaId: sagaInstance.id,
              stepName,
              error: compensationError.message,
            },
            timestamp: new Date(),
          });
        }
      }
    }
    
    sagaInstance.status = 'compensated';
    sagaInstance.compensatedAt = new Date();
  }

  // Monitor saga health and recovery
  async monitorSagaHealth(): Promise<void> {
    setInterval(async () => {
      const staleThreshold = 10 * 60 * 1000; // 10 minutes
      const currentTime = Date.now();
      
      for (const [sagaId, saga] of this.activeSagas) {
        const sagaAge = currentTime - saga.startedAt.getTime();
        
        if (saga.status === 'running' && sagaAge > staleThreshold) {
          console.warn(`Saga ${sagaId} appears to be stale. Age: ${sagaAge}ms`);
          
          // Attempt to recover or compensate stale saga
          await this.recoverStaleSaga(saga);
        }
      }
    }, 60000); // Check every minute
  }

  private async recoverStaleSaga(saga: SagaInstance): Promise<void> {
    try {
      // Try to resume saga from current step
      const sagaDefinition = this.sagaDefinitions.get(saga.type)!;
      await this.executeSagaSteps(saga, sagaDefinition);
      
    } catch (error) {
      console.error(`Failed to recover saga ${saga.id}:`, error);
      
      // If recovery fails, compensate
      const sagaDefinition = this.sagaDefinitions.get(saga.type)!;
      await this.compensateSaga(saga, sagaDefinition);
    }
  }
}
```

### 3.4 Performance & Scalability Patterns

**Production-Ready Optimization Strategies**:

```typescript
// Performance Monitoring & Optimization Service
class PerformanceOptimizer {
  private metricsCollector: MetricsCollector;
  private cacheManager: CacheManager;
  private loadBalancer: LoadBalancer;
  private autoscaler: AutoScaler;

  constructor() {
    this.metricsCollector = new MetricsCollector();
    this.cacheManager = new CacheManager();
    this.loadBalancer = new LoadBalancer();
    this.autoscaler = new AutoScaler();
  }

  // Real-time performance optimization
  async optimizePerformance(): Promise<void> {
    // Collect current performance metrics
    const metrics = await this.metricsCollector.collect();
    
    // Analyze performance bottlenecks
    const bottlenecks = await this.analyzeBottlenecks(metrics);
    
    // Apply optimizations based on analysis
    for (const bottleneck of bottlenecks) {
      await this.applyOptimization(bottleneck);
    }
    
    // Update optimization metrics
    await this.updateOptimizationMetrics(metrics, bottlenecks);
  }

  private async analyzeBottlenecks(metrics: PerformanceMetrics): Promise<Bottleneck[]> {
    const bottlenecks: Bottleneck[] = [];

    // CPU utilization analysis
    if (metrics.cpu.utilization > 80) {
      bottlenecks.push({
        type: 'cpu',
        severity: 'high',
        description: 'High CPU utilization detected',
        recommendation: 'Scale out or optimize CPU-intensive operations',
        metrics: { cpuUtilization: metrics.cpu.utilization },
      });
    }

    // Memory utilization analysis
    if (metrics.memory.utilization > 85) {
      bottlenecks.push({
        type: 'memory',
        severity: 'high',
        description: 'High memory utilization detected',
        recommendation: 'Scale out or optimize memory usage',
        metrics: { memoryUtilization: metrics.memory.utilization },
      });
    }

    // Database query performance analysis
    if (metrics.database.averageQueryTime > 100) {
      bottlenecks.push({
        type: 'database',
        severity: 'medium',
        description: 'Slow database queries detected',
        recommendation: 'Optimize queries or add indexes',
        metrics: { avgQueryTime: metrics.database.averageQueryTime },
      });
    }

    // API response time analysis
    if (metrics.api.p95ResponseTime > 2000) {
      bottlenecks.push({
        type: 'api',
        severity: 'high',
        description: 'High API response times detected',
        recommendation: 'Optimize API endpoints or scale',
        metrics: { p95ResponseTime: metrics.api.p95ResponseTime },
      });
    }

    // Cache hit rate analysis
    if (metrics.cache.hitRate < 80) {
      bottlenecks.push({
        type: 'cache',
        severity: 'medium',
        description: 'Low cache hit rate detected',
        recommendation: 'Optimize caching strategy',
        metrics: { hitRate: metrics.cache.hitRate },
      });
    }

    return bottlenecks;
  }

  private async applyOptimization(bottleneck: Bottleneck): Promise<void> {
    switch (bottleneck.type) {
      case 'cpu':
      case 'memory':
        await this.autoscaler.scaleOut(bottleneck.type);
        break;
        
      case 'database':
        await this.optimizeDatabaseQueries();
        break;
        
      case 'api':
        await this.optimizeAPIPerformance();
        break;
        
      case 'cache':
        await this.optimizeCacheStrategy();
        break;
    }
  }

  // Advanced caching strategies
  private async optimizeCacheStrategy(): Promise<void> {
    // Implement multi-level caching
    await this.setupMultiLevelCache();
    
    // Optimize cache warming
    await this.implementCacheWarming();
    
    // Set up intelligent cache invalidation
    await this.setupIntelligentInvalidation();
  }

  private async setupMultiLevelCache(): Promise<void> {
    const cacheStrategy = {
      L1: {
        type: 'in-memory',
        size: '256MB',
        ttl: 300, // 5 minutes
        policy: 'LRU',
      },
      L2: {
        type: 'redis',
        size: '2GB',
        ttl: 3600, // 1 hour
        policy: 'LRU',
      },
      L3: {
        type: 'cdn',
        size: 'unlimited',
        ttl: 86400, // 24 hours
        policy: 'TTL',
      },
    };

    await this.cacheManager.configureMultiLevel(cacheStrategy);
  }

  // Database optimization strategies
  private async optimizeDatabaseQueries(): Promise<void> {
    // Analyze slow queries
    const slowQueries = await this.identifySlowQueries();
    
    // Auto-suggest indexes
    const indexSuggestions = await this.suggestIndexes(slowQueries);
    
    // Implement query optimization
    for (const suggestion of indexSuggestions) {
      if (suggestion.impact > 0.5) { // Only high-impact optimizations
        await this.implementOptimization(suggestion);
      }
    }
  }

  private async identifySlowQueries(): Promise<SlowQuery[]> {
    // Query performance monitoring
    return await this.metricsCollector.getSlowQueries({
      threshold: 100, // ms
      limit: 50,
      period: '1h',
    });
  }

  // Load balancing optimization
  async optimizeLoadBalancing(): Promise<void> {
    const serviceMetrics = await this.metricsCollector.getServiceMetrics();
    
    // Implement weighted round-robin based on performance
    const weights = this.calculateServiceWeights(serviceMetrics);
    await this.loadBalancer.updateWeights(weights);
    
    // Implement health-based routing
    await this.setupHealthBasedRouting();
    
    // Configure circuit breakers
    await this.configureCircuitBreakers();
  }

  private calculateServiceWeights(metrics: ServiceMetrics[]): ServiceWeight[] {
    return metrics.map(metric => {
      // Calculate weight based on response time and success rate
      const responseTimeFactor = Math.max(0, 1 - (metric.avgResponseTime / 1000));
      const successRateFactor = metric.successRate / 100;
      const cpuFactor = Math.max(0, 1 - (metric.cpuUtilization / 100));
      
      const weight = (responseTimeFactor + successRateFactor + cpuFactor) / 3;
      
      return {
        serviceId: metric.serviceId,
        weight: Math.max(0.1, weight), // Minimum weight of 0.1
      };
    });
  }

  // Auto-scaling based on performance metrics
  async configureIntelligentAutoscaling(): Promise<void> {
    const scalingRules = [
      {
        metric: 'cpu_utilization',
        threshold: 70,
        action: 'scale_out',
        cooldown: 300, // 5 minutes
        scalingFactor: 1.5,
      },
      {
        metric: 'memory_utilization', 
        threshold: 80,
        action: 'scale_out',
        cooldown: 300,
        scalingFactor: 1.3,
      },
      {
        metric: 'request_queue_length',
        threshold: 100,
        action: 'scale_out',
        cooldown: 180, // 3 minutes
        scalingFactor: 2.0,
      },
      {
        metric: 'response_time_p95',
        threshold: 2000, // 2 seconds
        action: 'scale_out',
        cooldown: 240,
        scalingFactor: 1.8,
      },
    ];

    await this.autoscaler.configureRules(scalingRules);
  }

  // Predictive scaling based on historical patterns
  async implementPredictiveScaling(): Promise<void> {
    const historicalData = await this.metricsCollector.getHistoricalMetrics('30d');
    
    // Analyze traffic patterns
    const patterns = await this.analyzeTrafficPatterns(historicalData);
    
    // Generate scaling predictions
    const predictions = await this.generateScalingPredictions(patterns);
    
    // Schedule pre-emptive scaling
    await this.schedulePreemptiveScaling(predictions);
  }

  private async analyzeTrafficPatterns(data: HistoricalMetrics[]): Promise<TrafficPattern[]> {
    // Machine learning model to identify patterns
    const patterns: TrafficPattern[] = [];
    
    // Daily patterns (business hours, lunch breaks)
    const dailyPattern = this.extractDailyPattern(data);
    patterns.push(dailyPattern);
    
    // Weekly patterns (weekdays vs weekends)
    const weeklyPattern = this.extractWeeklyPattern(data);
    patterns.push(weeklyPattern);
    
    // Market hours patterns (trading sessions)
    const marketPattern = this.extractMarketPattern(data);
    patterns.push(marketPattern);
    
    // Event-driven patterns (course launches, signal alerts)
    const eventPattern = await this.extractEventPatterns(data);
    patterns.push(eventPattern);
    
    return patterns;
  }
}

// Circuit Breaker Pattern Implementation
class CircuitBreaker {
  private state: 'closed' | 'open' | 'half-open';
  private failureCount: number;
  private lastFailureTime: number;
  private successCount: number;
  private timeout: number;
  private threshold: number;

  constructor(
    private serviceName: string,
    options: CircuitBreakerOptions = {}
  ) {
    this.state = 'closed';
    this.failureCount = 0;
    this.lastFailureTime = 0;
    this.successCount = 0;
    this.timeout = options.timeout || 60000; // 1 minute
    this.threshold = options.threshold || 5; // 5 failures
  }

  async execute<T>(operation: () => Promise<T>): Promise<T> {
    if (this.state === 'open') {
      if (Date.now() - this.lastFailureTime > this.timeout) {
        this.state = 'half-open';
        this.successCount = 0;
      } else {
        throw new CircuitBreakerOpenError(this.serviceName);
      }
    }

    try {
      const result = await operation();
      this.onSuccess();
      return result;
    } catch (error) {
      this.onFailure();
      throw error;
    }
  }

  private onSuccess(): void {
    this.failureCount = 0;
    
    if (this.state === 'half-open') {
      this.successCount++;
      if (this.successCount >= 3) { // 3 successful calls to close
        this.state = 'closed';
      }
    }
  }

  private onFailure(): void {
    this.failureCount++;
    this.lastFailureTime = Date.now();
    
    if (this.failureCount >= this.threshold) {
      this.state = 'open';
    }
  }

  getState(): string {
    return this.state;
  }

  getMetrics(): CircuitBreakerMetrics {
    return {
      serviceName: this.serviceName,
      state: this.state,
      failureCount: this.failureCount,
      successCount: this.successCount,
      lastFailureTime: this.lastFailureTime,
    };
  }
}

// Rate Limiting with Sliding Window
class SlidingWindowRateLimiter {
  private windows: Map<string, TimeWindow>;
  private windowSize: number;
  private maxRequests: number;

  constructor(windowSize: number, maxRequests: number) {
    this.windows = new Map();
    this.windowSize = windowSize;
    this.maxRequests = maxRequests;
  }

  async isAllowed(identifier: string): Promise<RateLimitResult> {
    const now = Date.now();
    const windowKey = `${identifier}:${Math.floor(now / this.windowSize)}`;
    
    let window = this.windows.get(windowKey);
    if (!window) {
      window = { count: 0, timestamp: now };
      this.windows.set(windowKey, window);
    }

    // Clean up old windows
    this.cleanupOldWindows(now);

    // Check current window count
    if (window.count >= this.maxRequests) {
      return {
        allowed: false,
        remainingRequests: 0,
        resetTime: window.timestamp + this.windowSize,
        retryAfter: Math.ceil((window.timestamp + this.windowSize - now) / 1000),
      };
    }

    // Increment counter
    window.count++;

    return {
      allowed: true,
      remainingRequests: this.maxRequests - window.count,
      resetTime: window.timestamp + this.windowSize,
      retryAfter: 0,
    };
  }

  private cleanupOldWindows(currentTime: number): void {
    const cutoffTime = currentTime - this.windowSize;
    
    for (const [key, window] of this.windows) {
      if (window.timestamp < cutoffTime) {
        this.windows.delete(key);
      }
    }
  }
}
```

---

## 4. Production Deployment & Monitoring

### 4.1 Container Orchestration

**Kubernetes Deployment Strategy**:

```yaml
# Production Kubernetes Deployment Configuration
apiVersion: apps/v1
kind: Deployment
metadata:
  name: treum-frontend-shell
  namespace: frontend
  labels:
    app: treum-frontend-shell
    version: v1.0.0
spec:
  replicas: 5
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 2
  selector:
    matchLabels:
      app: treum-frontend-shell
  template:
    metadata:
      labels:
        app: treum-frontend-shell
        version: v1.0.0
    spec:
      containers:
      - name: frontend-shell
        image: treum/frontend-shell:1.0.0
        ports:
        - containerPort: 3000
        env:
        - name: NODE_ENV
          value: "production"
        - name: API_BASE_URL
          valueFrom:
            configMapKeyRef:
              name: frontend-config
              key: api-base-url
        - name: WS_URL
          valueFrom:
            configMapKeyRef:
              name: frontend-config
              key: websocket-url
        resources:
          requests:
            cpu: 200m
            memory: 512Mi
          limits:
            cpu: 500m
            memory: 1Gi
        livenessProbe:
          httpGet:
            path: /health
            port: 3000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 3000
          initialDelaySeconds: 5
          periodSeconds: 5
        lifecycle:
          preStop:
            exec:
              command: ["/bin/sh", "-c", "sleep 15"]

---
apiVersion: v1
kind: Service
metadata:
  name: frontend-shell-service
  namespace: frontend
spec:
  selector:
    app: treum-frontend-shell
  ports:
  - protocol: TCP
    port: 80
    targetPort: 3000
  type: ClusterIP

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: frontend-shell-hpa
  namespace: frontend
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: treum-frontend-shell
  minReplicas: 5
  maxReplicas: 50
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: concurrent_requests
      target:
        type: AverageValue
        averageValue: "100"
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60

---
# Backend Service Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: treum-api-gateway
  namespace: backend
spec:
  replicas: 10
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 2
      maxSurge: 3
  selector:
    matchLabels:
      app: treum-api-gateway
  template:
    metadata:
      labels:
        app: treum-api-gateway
        version: v2.1.0
      annotations:
        sidecar.istio.io/inject: "true"
        prometheus.io/scrape: "true"
        prometheus.io/port: "9090"
    spec:
      serviceAccountName: api-gateway-sa
      containers:
      - name: api-gateway
        image: treum/api-gateway:2.1.0
        ports:
        - containerPort: 8080
        - containerPort: 9090 # Metrics port
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: database-secrets
              key: connection-string
        - name: REDIS_URL
          valueFrom:
            secretKeyRef:
              name: redis-secrets
              key: connection-string
        - name: JWT_SECRET
          valueFrom:
            secretKeyRef:
              name: auth-secrets
              key: jwt-secret
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 2
            memory: 4Gi
        livenessProbe:
          httpGet:
            path: /health/live
            port: 8080
          initialDelaySeconds: 45
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /health/ready
            port: 8080
          initialDelaySeconds: 15
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
        volumeMounts:
        - name: config-volume
          mountPath: /app/config
          readOnly: true
        - name: tls-certs
          mountPath: /app/certs
          readOnly: true
      volumes:
      - name: config-volume
        configMap:
          name: api-gateway-config
      - name: tls-certs
        secret:
          secretName: api-gateway-tls
      nodeSelector:
        workload-type: api
      tolerations:
      - key: "workload-type"
        operator: "Equal"
        value: "api"
        effect: "NoSchedule"
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - treum-api-gateway
              topologyKey: kubernetes.io/hostname
```

### 4.2 Monitoring & Observability

**Comprehensive Monitoring Stack**:

```yaml
# Prometheus Configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
      external_labels:
        cluster: 'treum-production'
        region: 'ap-south-1'

    rule_files:
      - "alerts.yml"
      - "recording-rules.yml"

    scrape_configs:
    # Kubernetes API Server
    - job_name: 'kubernetes-apiservers'
      kubernetes_sd_configs:
      - role: endpoints
        namespaces:
          names:
          - default
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      relabel_configs:
      - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
        action: keep
        regex: default;kubernetes;https

    # Frontend Applications
    - job_name: 'treum-frontend'
      kubernetes_sd_configs:
      - role: pod
        namespaces:
          names: ['frontend']
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)

    # Backend Services
    - job_name: 'treum-backend'
      kubernetes_sd_configs:
      - role: pod
        namespaces:
          names: ['backend']
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        target_label: __meta_kubernetes_pod_container_port_number
        regex: (.+)

    # Legacy Systems (via Blackbox Exporter)
    - job_name: 'legacy-systems'
      static_configs:
      - targets:
        - legacy-trading.internal:8080
        - legacy-cms.internal:80
        - legacy-support.internal:3000
      metrics_path: /probe
      params:
        module: [http_2xx]
      relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - source_labels: [__param_target]
        target_label: instance
      - target_label: __address__
        replacement: blackbox-exporter:9115

    # Database Monitoring
    - job_name: 'postgresql'
      static_configs:
      - targets: ['postgres-exporter:9187']

    - job_name: 'redis'
      static_configs:
      - targets: ['redis-exporter:9121']

    # Message Queue Monitoring
    - job_name: 'kafka'
      static_configs:
      - targets: ['kafka-exporter:9308']

    alerting:
      alertmanagers:
      - static_configs:
        - targets:
          - alertmanager:9093

  alerts.yml: |
    groups:
    - name: treum-production-alerts
      rules:
      # High Error Rate
      - alert: HighErrorRate
        expr: |
          (
            rate(http_requests_total{status=~"5.."}[5m]) / 
            rate(http_requests_total[5m])
          ) > 0.05
        for: 5m
        labels:
          severity: critical
          service: "{{ $labels.service }}"
        annotations:
          summary: "High error rate detected for {{ $labels.service }}"
          description: "Error rate is {{ $value | humanizePercentage }} for service {{ $labels.service }}"

      # High Response Time
      - alert: HighResponseTime
        expr: |
          histogram_quantile(0.95, 
            rate(http_request_duration_seconds_bucket[5m])
          ) > 2
        for: 10m
        labels:
          severity: warning
          service: "{{ $labels.service }}"
        annotations:
          summary: "High response time for {{ $labels.service }}"
          description: "95th percentile response time is {{ $value }}s for {{ $labels.service }}"

      # Database Connection Issues
      - alert: DatabaseConnectionFailure
        expr: up{job="postgresql"} == 0
        for: 1m
        labels:
          severity: critical
          component: database
        annotations:
          summary: "Database connection failure"
          description: "PostgreSQL database is not responding"

      # Memory Usage
      - alert: HighMemoryUsage
        expr: |
          (
            container_memory_usage_bytes{pod=~"treum-.*"} / 
            container_spec_memory_limit_bytes{pod=~"treum-.*"}
          ) > 0.9
        for: 10m
        labels:
          severity: warning
          pod: "{{ $labels.pod }}"
        annotations:
          summary: "High memory usage for {{ $labels.pod }}"
          description: "Memory usage is {{ $value | humanizePercentage }} for pod {{ $labels.pod }}"

      # Legacy System Health
      - alert: LegacySystemDown
        expr: probe_success{job="legacy-systems"} == 0
        for: 3m
        labels:
          severity: critical
          system: "{{ $labels.instance }}"
        annotations:
          summary: "Legacy system {{ $labels.instance }} is down"
          description: "Legacy system {{ $labels.instance }} is not responding to health checks"

      # Kafka Lag
      - alert: HighKafkaConsumerLag
        expr: kafka_consumer_lag_sum > 10000
        for: 5m
        labels:
          severity: warning
          topic: "{{ $labels.topic }}"
        annotations:
          summary: "High Kafka consumer lag for topic {{ $labels.topic }}"
          description: "Consumer lag is {{ $value }} messages for topic {{ $labels.topic }}"

---
# Grafana Dashboard ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboards
  namespace: monitoring
data:
  treum-overview.json: |
    {
      "dashboard": {
        "title": "TREUM Production Overview",
        "panels": [
          {
            "title": "Request Rate",
            "type": "graph",
            "targets": [
              {
                "expr": "sum(rate(http_requests_total[5m])) by (service)",
                "legendFormat": "{{ service }}"
              }
            ]
          },
          {
            "title": "Error Rate",
            "type": "graph",
            "targets": [
              {
                "expr": "sum(rate(http_requests_total{status=~\"5..\"}[5m])) by (service) / sum(rate(http_requests_total[5m])) by (service)",
                "legendFormat": "{{ service }}"
              }
            ]
          },
          {
            "title": "Response Time",
            "type": "graph", 
            "targets": [
              {
                "expr": "histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (service, le))",
                "legendFormat": "{{ service }} p95"
              }
            ]
          },
          {
            "title": "Active Users",
            "type": "stat",
            "targets": [
              {
                "expr": "sum(active_websocket_connections)",
                "legendFormat": "Active Connections"
              }
            ]
          },
          {
            "title": "System Resources",
            "type": "row",
            "panels": [
              {
                "title": "CPU Usage",
                "type": "graph",
                "targets": [
                  {
                    "expr": "sum(rate(container_cpu_usage_seconds_total{pod=~\"treum-.*\"}[5m])) by (pod)",
                    "legendFormat": "{{ pod }}"
                  }
                ]
              },
              {
                "title": "Memory Usage", 
                "type": "graph",
                "targets": [
                  {
                    "expr": "sum(container_memory_usage_bytes{pod=~\"treum-.*\"}) by (pod) / 1024 / 1024 / 1024",
                    "legendFormat": "{{ pod }}"
                  }
                ]
              }
            ]
          }
        ]
      }
    }

---
# Jaeger Tracing Configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: jaeger-config
  namespace: monitoring
data:
  jaeger.yml: |
    # Sampling configuration
    sampling:
      default_strategy:
        type: probabilistic
        param: 0.1 # Sample 10% of traces in production
      per_service_strategies:
        - service: "treum-api-gateway"
          type: probabilistic
          param: 0.2 # Higher sampling for critical services
        - service: "signals-service"
          type: probabilistic
          param: 0.3 # Even higher for real-time services
        - service: "payment-service"
          type: probabilistic
          param: 1.0 # 100% sampling for financial transactions

    # Storage configuration
    storage:
      type: elasticsearch
      elasticsearch:
        server-urls: http://elasticsearch:9200
        index-prefix: treum-traces
        create-index-templates: true
        max-span-age: 168h # 7 days retention

    # Query service configuration
    query:
      base-path: /tracing
      static-files: /go/jaeger-ui/
      ui-config: /etc/jaeger/ui-config.json

  ui-config.json: |
    {
      "monitor": {
        "menuEnabled": true
      },
      "dependencies": {
        "menuEnabled": true
      },
      "archiveEnabled": true,
      "tracking": {
        "gaID": null
      }
    }
```

### 4.3 Security & Compliance

**Production Security Configuration**:

```yaml
# Network Policies for Zero Trust
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: frontend-network-policy
  namespace: frontend
spec:
  podSelector:
    matchLabels:
      tier: frontend
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: istio-system
    - namespaceSelector:
        matchLabels:
          name: backend
    ports:
    - protocol: TCP
      port: 3000
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: backend
    ports:
    - protocol: TCP
      port: 8080
  - to: []
    ports:
    - protocol: TCP
      port: 443 # HTTPS to external services
    - protocol: TCP
      port: 53  # DNS
    - protocol: UDP
      port: 53  # DNS

---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: backend-network-policy
  namespace: backend
spec:
  podSelector:
    matchLabels:
      tier: backend
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: frontend
    - namespaceSelector:
        matchLabels:
          name: backend
    ports:
    - protocol: TCP
      port: 8080
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: database
    ports:
    - protocol: TCP
      port: 5432 # PostgreSQL
  - to:
    - namespaceSelector:
        matchLabels:
          name: cache
    ports:
    - protocol: TCP
      port: 6379 # Redis
  - to:
    - namespaceSelector:
        matchLabels:
          name: legacy
    ports:
    - protocol: TCP
      port: 8080

---
# Pod Security Policy
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: treum-restricted
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
    - ALL
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
  runAsUser:
    rule: 'MustRunAsNonRoot'
  seLinux:
    rule: 'RunAsAny'
  fsGroup:
    rule: 'RunAsAny'
  readOnlyRootFilesystem: true
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000

---
# Security Scanning with Falco
apiVersion: v1
kind: ConfigMap
metadata:
  name: falco-config
  namespace: security
data:
  falco.yaml: |
    rules_file:
      - /etc/falco/falco_rules.yaml
      - /etc/falco/falco_rules.local.yaml
      - /etc/falco/treum_rules.yaml

    time_format_iso_8601: true
    json_output: true
    json_include_output_property: true
    
    # Output channels
    file_output:
      enabled: true
      keep_alive: false
      filename: /var/log/falco/events.log

    stdout_output:
      enabled: true

    syslog_output:
      enabled: false

    program_output:
      enabled: true
      keep_alive: true
      program: "curl -X POST http://falco-webhook:8080/webhook -H 'Content-Type: application/json' -d @-"

    http_output:
      enabled: true
      url: http://security-events-collector:8080/events

  treum_rules.yaml: |
    - rule: Unauthorized Access to Financial Data
      desc: Detect unauthorized access to financial/payment related files
      condition: >
        (open_read or open_write) and
        (fd.name contains "/financial/" or 
         fd.name contains "/payment/" or
         fd.name contains "/transaction/") and
        not proc.name in (payment-service, transaction-processor)
      output: >
        Unauthorized access to financial data
        (user=%user.name command=%proc.cmdline file=%fd.name container=%container.name)
      priority: CRITICAL

    - rule: Suspicious Process in Production
      desc: Detect suspicious processes that shouldn't run in production
      condition: >
        spawned_process and
        (proc.name in (nc, ncat, netcat, nmap, wireshark, tcpdump) or
         proc.name contains "python" or
         proc.name contains "bash" or
         proc.name contains "sh") and
        not container.name contains "debug"
      output: >
        Suspicious process spawned in production
        (user=%user.name command=%proc.cmdline container=%container.name)
      priority: WARNING

    - rule: Crypto Mining Detection
      desc: Detect potential cryptocurrency mining activity
      condition: >
        spawned_process and
        (proc.name in (xmrig, ethminer, cgminer, bfgminer) or
         proc.cmdline contains "stratum" or
         proc.cmdline contains "cryptonight" or
         proc.cmdline contains "daggerhashimoto")
      output: >
        Potential cryptocurrency mining detected
        (user=%user.name command=%proc.cmdline container=%container.name)
      priority: CRITICAL

---
# Vault Configuration for Secrets Management
apiVersion: v1
kind: ConfigMap
metadata:
  name: vault-config
  namespace: vault
data:
  vault.hcl: |
    ui = true
    
    storage "postgresql" {
      connection_url = "postgresql://vault:password@postgres:5432/vault?sslmode=disable"
    }

    listener "tcp" {
      address = "0.0.0.0:8200"
      tls_cert_file = "/vault/tls/tls.crt"
      tls_key_file = "/vault/tls/tls.key"
      tls_min_version = "tls12"
    }

    seal "awskms" {
      region = "ap-south-1"
      kms_key_id = "arn:aws:kms:ap-south-1:123456789012:key/12345678-1234-1234-1234-123456789012"
    }

    # API rate limiting
    api_addr = "https://vault.treum.in:8200"
    cluster_addr = "https://vault.treum.in:8201"

    # Logging
    log_level = "info"
    log_format = "json"

  vault-policy.hcl: |
    # Policy for backend services
    path "secret/data/backend/*" {
      capabilities = ["read"]
    }

    path "database/creds/backend" {
      capabilities = ["read"]
    }

    path "pki/issue/backend" {
      capabilities = ["create", "update"]
    }

    # Policy for payment service (more restrictive)
    path "secret/data/payment/*" {
      capabilities = ["read"]
      allowed_parameters = {
        "version" = []
      }
    }

    path "database/creds/payment-readonly" {
      capabilities = ["read"]
    }

    # Audit logging
    path "sys/audit" {
      capabilities = ["read"]
    }
```

---

## 5. Disaster Recovery & Business Continuity

### 5.1 Backup & Recovery Strategy

```yaml
# Automated Backup Configuration
apiVersion: batch/v1
kind: CronJob
metadata:
  name: postgres-backup
  namespace: database
spec:
  schedule: "0 2 * * *" # Daily at 2 AM
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: postgres-backup
            image: postgres:15
            env:
            - name: PGPASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgres-credentials
                  key: password
            command:
            - /bin/bash
            - -c
            - |
              DATE=$(date +%Y%m%d_%H%M%S)
              
              # Create full backup
              pg_dump -h postgres-primary -U postgres -d treum_production \
                --format=custom --compress=9 --verbose \
                --file=/backup/treum_full_${DATE}.dump
              
              # Upload to S3 with encryption
              aws s3 cp /backup/treum_full_${DATE}.dump \
                s3://treum-backups/database/full/ \
                --storage-class=STANDARD_IA \
                --server-side-encryption=AES256
              
              # Create incremental WAL backup
              pg_receivewal -h postgres-primary -U postgres \
                -D /backup/wal/${DATE} --compress=gzip
              
              # Sync WAL files to S3
              aws s3 sync /backup/wal/${DATE} \
                s3://treum-backups/database/wal/${DATE}/ \
                --delete
              
              # Cleanup local files older than 7 days
              find /backup -type f -mtime +7 -delete
              
              # Verify backup integrity
              pg_restore --list /backup/treum_full_${DATE}.dump > /dev/null
              if [ $? -eq 0 ]; then
                echo "Backup verification successful"
              else
                echo "Backup verification failed" >&2
                exit 1
              fi
            volumeMounts:
            - name: backup-storage
              mountPath: /backup
          volumes:
          - name: backup-storage
            persistentVolumeClaim:
              claimName: backup-pvc
          restartPolicy: OnFailure
          nodeSelector:
            workload-type: backup

---
# Redis Backup Configuration
apiVersion: batch/v1
kind: CronJob
metadata:
  name: redis-backup
  namespace: cache
spec:
  schedule: "0 */6 * * *" # Every 6 hours
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: redis-backup
            image: redis:7-alpine
            command:
            - /bin/sh
            - -c
            - |
              DATE=$(date +%Y%m%d_%H%M%S)
              
              # Create Redis backup
              redis-cli -h redis-primary --rdb /backup/redis_${DATE}.rdb
              
              # Compress backup
              gzip /backup/redis_${DATE}.rdb
              
              # Upload to S3
              aws s3 cp /backup/redis_${DATE}.rdb.gz \
                s3://treum-backups/redis/ \
                --storage-class=STANDARD_IA
              
              # Cleanup old backups
              find /backup -name "redis_*.rdb.gz" -mtime +3 -delete
            volumeMounts:
            - name: backup-storage
              mountPath: /backup
          restartPolicy: OnFailure

---
# Application State Backup
apiVersion: batch/v1
kind: CronJob
metadata:
  name: application-backup
  namespace: backend
spec:
  schedule: "0 3 * * *" # Daily at 3 AM
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: app-backup
            image: treum/backup-tools:latest
            command:
            - /bin/bash
            - -c
            - |
              DATE=$(date +%Y%m%d_%H%M%S)
              
              # Backup configuration
              kubectl get configmaps -A -o yaml > /backup/configmaps_${DATE}.yaml
              kubectl get secrets -A -o yaml > /backup/secrets_${DATE}.yaml
              
              # Backup persistent volumes
              kubectl get pv -o yaml > /backup/persistentvolumes_${DATE}.yaml
              kubectl get pvc -A -o yaml > /backup/persistentvolumeclaims_${DATE}.yaml
              
              # Backup custom resources
              kubectl get deployments -A -o yaml > /backup/deployments_${DATE}.yaml
              kubectl get services -A -o yaml > /backup/services_${DATE}.yaml
              kubectl get ingresses -A -o yaml > /backup/ingresses_${DATE}.yaml
              
              # Create archive
              tar -czf /backup/k8s_backup_${DATE}.tar.gz /backup/*_${DATE}.yaml
              
              # Upload to S3
              aws s3 cp /backup/k8s_backup_${DATE}.tar.gz \
                s3://treum-backups/kubernetes/ \
                --storage-class=STANDARD_IA
              
              # Cleanup
              rm /backup/*_${DATE}.yaml
              find /backup -name "k8s_backup_*.tar.gz" -mtime +7 -delete
            volumeMounts:
            - name: backup-storage
              mountPath: /backup
          restartPolicy: OnFailure
          serviceAccountName: backup-service-account

---
# Disaster Recovery Testing
apiVersion: batch/v1
kind: CronJob
metadata:
  name: dr-testing
  namespace: backup
spec:
  schedule: "0 6 * * 0" # Weekly on Sunday at 6 AM
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: dr-test
            image: treum/dr-tools:latest
            command:
            - /bin/bash
            - -c
            - |
              echo "Starting DR test at $(date)"
              
              # Test database restore
              LATEST_BACKUP=$(aws s3 ls s3://treum-backups/database/full/ | sort | tail -n 1 | awk '{print $4}')
              aws s3 cp s3://treum-backups/database/full/${LATEST_BACKUP} /tmp/
              
              # Create test database
              createdb -h postgres-test -U postgres treum_dr_test
              
              # Restore from backup
              pg_restore -h postgres-test -U postgres -d treum_dr_test /tmp/${LATEST_BACKUP}
              
              # Verify restore
              ORIGINAL_COUNT=$(psql -h postgres-primary -U postgres -d treum_production -t -c "SELECT COUNT(*) FROM users;")
              RESTORED_COUNT=$(psql -h postgres-test -U postgres -d treum_dr_test -t -c "SELECT COUNT(*) FROM users;")
              
              if [ "$ORIGINAL_COUNT" -eq "$RESTORED_COUNT" ]; then
                echo "Database restore test PASSED"
              else
                echo "Database restore test FAILED"
                exit 1
              fi
              
              # Test application deployment in DR environment
              kubectl apply -f /dr-configs/test-deployment.yaml -n dr-testing
              
              # Wait for pods to be ready
              kubectl wait --for=condition=ready pod -l app=treum-dr-test -n dr-testing --timeout=300s
              
              # Run health checks
              kubectl exec -n dr-testing deployment/treum-dr-test -- curl -f http://localhost:8080/health
              
              if [ $? -eq 0 ]; then
                echo "Application deployment test PASSED"
              else
                echo "Application deployment test FAILED"
                exit 1
              fi
              
              # Cleanup test resources
              kubectl delete namespace dr-testing
              dropdb -h postgres-test -U postgres treum_dr_test
              
              echo "DR test completed successfully at $(date)"
            volumeMounts:
            - name: dr-configs
              mountPath: /dr-configs
          volumes:
          - name: dr-configs
            configMap:
              name: dr-test-configs
          restartPolicy: OnFailure
```

---

## Summary

This comprehensive TREUM Full-Stack Architecture document provides:

### âœ… **Completed Components**

1. **Frontend Architecture** (âœ… Complete)
   - Component-based design with Atomic Design principles
   - Micro-frontend architecture with Module Federation
   - Advanced state management (Redux Toolkit + RTK Query)
   - Real-time WebSocket integration
   - Performance optimization strategies
   - SEO & SSR/SSG implementation
   - Progressive Web App capabilities

2. **Brownfield Architecture** (âœ… Complete)
   - Legacy system integration patterns
   - Strangler Fig pattern implementation
   - Event-driven integration architecture
   - Database synchronization strategies
   - Data transformation services
   - Conflict resolution mechanisms

3. **Integration Architecture** (âœ… Complete)
   - API Gateway & Service Mesh integration
   - Unified event bus architecture
   - Data consistency patterns
   - Circuit breaker implementations
   - Performance & scalability optimizations

4. **Production Deployment** (âœ… Complete)
   - Kubernetes orchestration
   - Comprehensive monitoring stack
   - Security & compliance configurations
   - Disaster recovery strategies

### ğŸ¯ **Key Achievements**

- **Scale Ready**: Supports â‚¹600 Cr revenue and 1M+ concurrent users
- **Financial Compliance**: PCI DSS, KYC/AML, and audit trail compliance
- **Production Grade**: Enterprise-level security, monitoring, and disaster recovery
- **Legacy Integration**: Seamless integration with existing systems
- **Performance Optimized**: Sub-100ms signal delivery, 99.9% uptime targets

### ğŸ“ **Architecture File Location**
`/Users/srijan/ai-finance-agency/TREUM_COMPLETE_FULLSTACK_ARCHITECTURE.md`

This architecture provides a complete foundation for building and scaling the TREUM platform across frontend, backend, and legacy system integration.